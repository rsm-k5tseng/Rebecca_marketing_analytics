[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kuan-Ling Tseng (Rebecca)",
    "section": "",
    "text": "Welcome!!!"
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the experiment, the authors collaborated with a nonprofit organization and conducted a large-scale field experiment involving over 50,000 prior donors. Each donor was randomly assigned to receive one of several types of direct-mail fundraising letters.\nThe control group received a standard letter with no match offer, while the treatment groups received a matching grant offer, where an anonymous donor pledged to match donations at a rate of 1:1, 2:1, or 3:1. Additionally, the letters varied in their presentation of the suggested donation amount and the total match cap ($25,000 / $50,000 / $100,000 / unstated).\nThe researchers then tracked each recipient’s response—specifically: • Whether they donated at all (gave) • How much they donated (amount)\nThis allowed the authors to examine whether matching grants increase donations, whether the size of the match ratio matters, and how the differences influence donor behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the experiment, the authors collaborated with a nonprofit organization and conducted a large-scale field experiment involving over 50,000 prior donors. Each donor was randomly assigned to receive one of several types of direct-mail fundraising letters.\nThe control group received a standard letter with no match offer, while the treatment groups received a matching grant offer, where an anonymous donor pledged to match donations at a rate of 1:1, 2:1, or 3:1. Additionally, the letters varied in their presentation of the suggested donation amount and the total match cap ($25,000 / $50,000 / $100,000 / unstated).\nThe researchers then tracked each recipient’s response—specifically: • Whether they donated at all (gave) • How much they donated (amount)\nThis allowed the authors to examine whether matching grants increase donations, whether the size of the match ratio matters, and how the differences influence donor behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\ndf = pd.read_stata(\"./data/karlan_list_2007.dta\", iterator=False)\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nfrom sklearn import linear_model\nimport numpy as np\n\ntest_variables = [\"hpa\", \"mrm2\", \"freq\", \"years\", \"year5\", \"dormant\"]\n\n\ndef compare_ttest_regress(variable):\n\n    subset = df[[\"treatment\", variable]].dropna()\n    treat = subset[subset[\"treatment\"] == 1][variable]\n    control = subset[subset[\"treatment\"] == 0][variable]\n\n    # t-test\n    mean_diff = treat.mean() - control.mean()\n    var_treat = treat.var(ddof=1)\n    var_control = control.var(ddof=1)\n    se = np.sqrt(var_treat / len(treat) + var_control / len(control))\n    t_stat = mean_diff / se\n    ttest_result = round(t_stat, 2)\n\n    # regression\n    X = subset[\"treatment\"].values.reshape(-1, 1)\n    y = subset[variable].values\n    reg = linear_model.LinearRegression()\n    reg.fit(X, y)\n    reg_result = round(reg.coef_[0] / se, 2)\n\n    return ttest_result, reg_result\n\n\nresults = []\nfor v in test_variables:\n    results.append(\n        {\n            \"variable\": v,\n            \"t_test\": compare_ttest_regress(v)[0],\n            \"regress\": compare_ttest_regress(v)[1],\n        }\n    )\n\ndf_result = pd.DataFrame(results)\ndf_result\n\n\n\n\n\n\n\n\nvariable\nt_test\nregress\n\n\n\n\n0\nhpa\n0.97\n0.97\n\n\n1\nmrm2\n0.12\n0.12\n\n\n2\nfreq\n-0.11\n-0.11\n\n\n3\nyears\n-1.09\n-1.09\n\n\n4\nyear5\n-1.56\n-1.56\n\n\n5\ndormant\n0.17\n0.17"
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\n\ndf_treat = df.query(\"treatment==1\")\ndf_control = df.query(\"control==1\")\ntreat_prop = df_treat[\"gave\"].value_counts(normalize=True)\ncontrol_prop = df_control[\"gave\"].value_counts(normalize=True)\n\nfig, ax = plt.subplots()\nbars = ax.bar(\n    [\"treat\", \"control\"],\n    [treat_prop[1] * 100, control_prop[1] * 100],\n    color=[\"skyblue\", \"orange\"],\n)\n\nfor bar in bars:\n    height = bar.get_height()\n    ax.annotate(\n        f\"{height:.1f}%\",\n        xy=(bar.get_x() + bar.get_width() / 2, height),\n        xytext=(0, 2),\n        textcoords=\"offset points\",\n        ha=\"center\",\n        va=\"bottom\",\n    )\n\n\n\n\n\n\n\n\n\ntreat = df[df[\"treatment\"] == 1][\"gave\"]\ncontrol = df[df[\"treatment\"] == 0][\"gave\"]\n# t-test\nmean_diff = treat.mean() - control.mean()\nvar_treat = treat.var(ddof=1)\nvar_control = control.var(ddof=1)\nse = np.sqrt(var_treat / len(treat) + var_control / len(control))\nt_stat = mean_diff / se\nttest_result = round(t_stat, 2)\nttest_result\n\nnp.float64(3.21)\n\n\nSince the t-test result is so large, we can conclude that the treatment group is significantly more willing to donate than the control group.\n\n# regression\nX = df[\"treatment\"].values.reshape(-1, 1)\ny = df[\"gave\"].values\nreg = linear_model.LinearRegression()\nreg.fit(X, y)\nreg_result = round(reg.coef_[0] / se, 2)\nreg_result\n\nnp.float64(3.21)\n\n\nAccording to the linear regression model, the coefficient is so big that we can conclude that the treatment group is more likely to donate than the control group.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\ndf_clean = df[[\"gave\", \"treatment\"]].dropna()\n\nX = sm.add_constant(df_clean[\"treatment\"])\ny = df_clean[\"gave\"]\nprobit_model = sm.Probit(y, X).fit()\n\nprint(probit_model.summary())\n\nmfx = probit_model.get_margeff()\nprint(mfx.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 23 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        10:38:22   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\nBased on the probit regression result, people in the treatment group are significantly more inclined to donate money.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nfrom scipy import stats\n\nratio1 = df.query(\"ratio==1\")[\"gave\"]\nratio2 = df.query(\"ratio2==1\")[\"gave\"]\nratio3 = df.query(\"ratio3==1\")[\"gave\"]\n\n\n# t-test\ndef t_test(df1, df2):\n    mean_diff = df1.mean() - df2.mean()\n    var1 = df1.var(ddof=1)\n    var2 = df2.var(ddof=1)\n    se = np.sqrt(var1 / len(df1) + var2 / len(df2))\n    t_stat = mean_diff / se\n    ttest_result = round(t_stat, 2)\n    degree_of_freedom = (var1 / len(df1) + var1 / len(df2)) ** 2 / (\n        ((var1 / len(df1)) ** 2) / (len(df1) - 1)\n        + ((var1 / len(df2)) ** 2) / (len(df2) - 1)\n    )\n\n    p_value = stats.t.sf(np.abs(t_stat), degree_of_freedom) * 2\n\n    return ttest_result, round(p_value, 2)\n\n\nprint(\n    f\"1:1 vs 2:1: t-test {t_test(ratio2, ratio1)[0]}, p-value {t_test(ratio2, ratio1)[1]}\"\n)\nprint(\n    f\"2:1 vs 3:1: t-test {t_test(ratio3, ratio2)[0]}, p-value {t_test(ratio3, ratio2)[1]}\"\n)\nprint(\n    f\"1:1 vs 3:1: t-test {t_test(ratio3, ratio1)[0]}, p-value {t_test(ratio3, ratio1)[1]}\"\n)\n\n1:1 vs 2:1: t-test 0.97, p-value 0.33\n2:1 vs 3:1: t-test 0.05, p-value 0.96\n1:1 vs 3:1: t-test 1.02, p-value 0.31\n\n\nThe results show that there are no significant difference between match ratios, 1:1 vs 2:1, 2:1 vs 3:1, or 1:1 vs 3:1.\n\nX = df[[\"ratio2\", \"ratio3\"]]\nX = sm.add_constant(X)\ny = df[\"gave\"]\n\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     4.117\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0163\nTime:                        10:38:22   Log-Likelihood:                 26629.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50080   BIC:                        -5.323e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0190      0.001     22.306      0.000       0.017       0.021\nratio2         0.0036      0.002      2.269      0.023       0.000       0.007\nratio3         0.0037      0.002      2.332      0.020       0.001       0.007\n==============================================================================\nOmnibus:                    59815.856   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317637.927\nSkew:                           6.741   Prob(JB):                         0.00\nKurtosis:                      46.443   Cond. No.                         3.16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nratio1_mean = df.query(\"(ratio2 == 0) & (ratio3 == 0)\")[\"gave\"].mean()\nratio2_mean = df.query(\"ratio2==1\")[\"gave\"].mean()\nratio3_mean = df.query(\"ratio3==1\")[\"gave\"].mean()\n\ndiff11_21 = ratio2_mean - ratio1_mean\ndiff21_31 = ratio3_mean - ratio2_mean\n\nprint(f\"2:1 - 1:1 = {diff11_21:.4f}\")\nprint(f\"3:1 - 2:1 = {diff21_31:.4f}\")\n\n2:1 - 1:1 = 0.0036\n3:1 - 2:1 = 0.0001\n\n\nWhen calculating the response rate directly from the data, the difference between the 2:1 and 1:1 match ratios is about 0.36, while the difference between 3:1 and 2:1 is only 0.01. These results are consistent with the coefficients from the OLS regression, where ratio2 and ratio3 have coefficients of 0.0036 and 0.0037 respectively.\nIn conclusion, increasing the match ratio from 1:1 to 2:1 appears to have a small positive effect on donation likelihood, but increasing the match ratio further to 3:1 shows no additional gain.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nT-test\n\ntreat_amount = df.query(\"treatment == 1\")[\"amount\"]\ncontrol_amount = df.query(\"control == 1\")[\"amount\"]\nprint(\n    f\"t_stats: {t_test(treat_amount, control_amount)[0]:.2f}, p-value: {t_test(treat_amount, control_amount)[1]:.2f}\"\n)\n\nt_stats: 1.92, p-value: 0.06\n\n\nLinear Regression\n\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"amount\"]\nmodel = sm.OLS(y, X, missing=\"drop\").fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0628\nTime:                        10:38:22   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe conduct a t-test and a bivariate regression of the donation amount on treatment assignment. The results show that treatment group donors gave approximately $0.15 more on average than the control group, but the difference is small that the p-value wasn’t statistically significant, meaning that the impact is limited.\n\ntreat_amount_gave = df.query(\"treatment == 1 and gave==1\")[\"amount\"]\ncontrol_amount_gave = df.query(\"control == 1 and gave==1\")[\"amount\"]\nX = sm.add_constant(df.query(\"gave==1\")[\"treatment\"])\ny = df.query(\"gave==1\")[\"amount\"]\nmodel = sm.OLS(y, X, missing=\"drop\").fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.561\nTime:                        10:38:22   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe then looked into the sample that made a donation and did the regression analysis o donation amount on treatment assignment. The coefficient on treatment is −1.67 with a p-value of 0.561, indicating no statistically significant difference in donation amounts between the treatment and control groups, suggesting that the treatment group only, on average, gave slightly less than the control group.\nHowever, the coefficient does not allow a causal interpretation because adding conditions on treatment behavior (donating) introduces potential selection bias.\n\nfig, ax = plt.subplots()\nbars = ax.bar(\n    [\"treat\", \"control\"],\n    [treat_amount_gave.mean(), control_amount_gave.mean()],\n    color=[\"skyblue\", \"orange\"],\n)\n\nfor bar in bars:\n    height = bar.get_height()\n    ax.annotate(\n        f\"${height:.2f}\",\n        xy=(bar.get_x() + bar.get_width() / 2, height),\n        xytext=(0, 2),\n        textcoords=\"offset points\",\n        ha=\"center\",\n        va=\"bottom\",\n    )\n\nax.set_ylabel(\"Average Donation Amount ($)\")\nax.set_title(\"Average Donation Amount\")\n\nplt.show()\n\n\n\n\n\n\n\n\nThe bar chart compares the average donation amounts between treatment and control groups, conditional on having donated. The control group gave slightly more on average ($45.54) than the treatment group ($43.87)."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\np_control = 0.018\np_treat = 0.022\nn_sim = 10_000\n\nnp.random.seed(100)\ncontrol_draw = np.random.binomial(1, p_control, n_sim)\ntreat_draw = np.random.binomial(1, p_treat, n_sim)\n\ndiff = treat_draw - control_draw\ncumulative_avg = np.cumsum(diff) / np.arange(1, n_sim + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label=\"Cumulative Avg of Difference: Treatment - Control\")\nplt.axhline(p_treat - p_control, color=\"red\", linestyle=\"--\", label=\"True Mean Diff\")\nplt.title(\"Simulation of Cumulative Average Difference\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe figure shows the cumulative average difference between treatment and control groups from a simulation. The average converges toward the true difference (0.004) as the number of simulations gets larger.\nThis illustrates the Law of Large Numbers: as the number of simulations increases, the observed mean gets closer to the expected (true) mean.\n\n\nCentral Limit Theorem\n\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    np.random.seed(100)\n    for _ in range(n_sim):\n        c = np.random.binomial(1, p_control, n).mean()\n        t = np.random.binomial(1, p_treat, n).mean()\n        diffs.append(t - c)\n\n    axs[i].hist(diffs, bins=30, color=\"lightblue\", edgecolor=\"black\")\n    axs[i].axvline(0, color=\"red\", linestyle=\"--\", label=\"Zero Line\")\n    axs[i].axvline(np.mean(diffs), color=\"green\", linestyle=\"-\", label=\"Mean Diff\")\n    axs[i].set_title(f\"Sample size = {n}\")\n    axs[i].set_xlabel(\"Treatment - Control Mean Diff\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\nplt.suptitle(\"Central Limit Theorem: Distribution of Mean Differences\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAs the sample size gets larger (50, 200, 500, 1,000), the distribution of average differences will be narrower, more symmetric, and closer to normal distribution.\nFor small samples (e.g., n = 50), the distribution is noisy and is closer to 0. As n becomes larger (e.g., n = 1,000), the distribution converges to normal and centers around the true effect (0.04)."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "Projects/project1/index.html",
    "href": "Projects/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment collaborated with a nonprofit organization involving over 50,000 prior donors. Each donor was randomly assigned to receive one of several types of direct-mail fundraising letters.\nThe control group received a standard letter with no match offer, while the treatment groups received a matching grant offer, where an anonymous donor pledged to match donations at a rate of 1:1, 2:1, or 3:1. Additionally, the letters varied in their presentation of the suggested donation amount and the total match maximum ($25,000 / $50,000 / $100,000 / unstated).\nThe researchers then tracked each recipient’s response—specifically:\n\nWhether they donated at all (gave)\nHow much they donated (amount)\n\nThis allowed the authors to examine whether matching grants increase donations, whether the size of the match ratio matters, and how the differences influence donor behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "Projects/project1/index.html#introduction",
    "href": "Projects/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment collaborated with a nonprofit organization involving over 50,000 prior donors. Each donor was randomly assigned to receive one of several types of direct-mail fundraising letters.\nThe control group received a standard letter with no match offer, while the treatment groups received a matching grant offer, where an anonymous donor pledged to match donations at a rate of 1:1, 2:1, or 3:1. Additionally, the letters varied in their presentation of the suggested donation amount and the total match maximum ($25,000 / $50,000 / $100,000 / unstated).\nThe researchers then tracked each recipient’s response—specifically:\n\nWhether they donated at all (gave)\nHow much they donated (amount)\n\nThis allowed the authors to examine whether matching grants increase donations, whether the size of the match ratio matters, and how the differences influence donor behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "Projects/project1/index.html#data",
    "href": "Projects/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset contains 51 columns, with 50,083 rows (mails sent). There are 33,396 mails sent to the treatment group, while 16,687 mails sent to the control group. As for the match ratio, 11,133 mails got 1:1, 11,134 mails got 2:1, and 11,129 mails got 3:1.\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_stata(\n    \"../../data/karlan_list_2007.dta\",\n    iterator=False,\n)\nprint(f\"Rows: {df.shape[0]:,}, Columns: {df.shape[1]:,}\")\nprint(f\"Treatment group: {df.treatment.sum():,}, Contol group: {df.control.sum():,}\")\nprint(\n    f\"Ratio 1:1: {sum(df.ratio == 1):,}, Ratio 2:1: {sum(df.ratio2 == 1):,}, Ratio 3:1: {sum(df.ratio3 == 1):,}\"\n)\ndf.head()\n\n\nRows: 50,083, Columns: 51\nTreatment group: 33,396, Contol group: 16,687\nRatio 1:1: 11,133, Ratio 2:1: 11,134, Ratio 3:1: 11,129\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nSelected Variables:hpa, mrm2, freq, years, year5, dormant\nFor the selected variables, I did T-test and regression to examine the difference between two groups - control group and treatment group.\n\n\nCode\nfrom sklearn import linear_model\nimport numpy as np\n\ntest_variables = [\"hpa\", \"mrm2\", \"freq\", \"years\", \"year5\", \"dormant\"]\n\n\ndef compare_ttest_regress(variable):\n\n    subset = df[[\"treatment\", variable]].dropna()\n    treat = subset[subset[\"treatment\"] == 1][variable]\n    control = subset[subset[\"treatment\"] == 0][variable]\n\n    # t-test\n    mean_diff = treat.mean() - control.mean()\n    var_treat = treat.var(ddof=1)\n    var_control = control.var(ddof=1)\n    se = np.sqrt(var_treat / len(treat) + var_control / len(control))\n    t_stat = mean_diff / se\n    ttest_result = round(t_stat, 2)\n\n    # regression\n    X = subset[\"treatment\"].values.reshape(-1, 1)\n    y = subset[variable].values\n    reg = linear_model.LinearRegression()\n    reg.fit(X, y)\n    reg_result = round(reg.coef_[0] / se, 2)\n\n    return treat.mean(), control.mean(), mean_diff, ttest_result, reg_result\n\n\nresults = []\nfor v in test_variables:\n    results.append(\n        {\n            \"variable\": v,\n            \"control mean\": compare_ttest_regress(v)[0],\n            \"treatment mean\": compare_ttest_regress(v)[1],\n            \"difference\": compare_ttest_regress(v)[2],\n            \"t_test\": compare_ttest_regress(v)[3],\n            \"regress\": compare_ttest_regress(v)[4],\n        }\n    )\n\npd.set_option(\"display.float_format\", \"{:.3f}\".format)\n\ndf_result = pd.DataFrame(results)\ndf_result\n\n\n\n\n\n\n\n\n\nvariable\ncontrol mean\ntreatment mean\ndifference\nt_test\nregress\n\n\n\n\n0\nhpa\n59.597\n58.960\n0.637\n0.970\n0.970\n\n\n1\nmrm2\n13.012\n12.998\n0.014\n0.120\n0.120\n\n\n2\nfreq\n8.035\n8.047\n-0.012\n-0.110\n-0.110\n\n\n3\nyears\n6.078\n6.136\n-0.058\n-1.090\n-1.090\n\n\n4\nyear5\n0.506\n0.514\n-0.007\n-1.560\n-1.560\n\n\n5\ndormant\n0.524\n0.523\n0.001\n0.170\n0.170\n\n\n\n\n\n\n\nAcross all selected variables, the differences between treatment and control groups are small. The t-test statistics and the regression coefficients for all variables are also small, indicating that the differences between the two groups are not statistically significant.\nThere is no evidence that there are selecting bias existing, which supports the validity of the experimental design and suggests that the selection was successfully random."
  },
  {
    "objectID": "Projects/project1/index.html#experimental-results",
    "href": "Projects/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\nCode\nimport matplotlib.pyplot as plt\n\n\ndf_treat = df.query(\"treatment==1\")\ndf_control = df.query(\"control==1\")\ntreat_prop = df_treat[\"gave\"].value_counts(normalize=True)\ncontrol_prop = df_control[\"gave\"].value_counts(normalize=True)\n\nfig, ax = plt.subplots()\nbars = ax.bar(\n    [\"treatment\", \"control\"],\n    [treat_prop[1] * 100, control_prop[1] * 100],\n    color=[\"skyblue\", \"orange\"],\n)\n\nfor bar in bars:\n    height = bar.get_height()\n    ax.annotate(\n        f\"{height:.1f}%\",\n        xy=(bar.get_x() + bar.get_width() / 2, height),\n        xytext=(0, 2),\n        textcoords=\"offset points\",\n        ha=\"center\",\n        va=\"bottom\",\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\ntreat = df[df[\"treatment\"] == 1][\"gave\"]\ncontrol = df[df[\"treatment\"] == 0][\"gave\"]\n# t-test\nmean_diff = treat.mean() - control.mean()\nvar_treat = treat.var(ddof=1)\nvar_control = control.var(ddof=1)\nse = np.sqrt(var_treat / len(treat) + var_control / len(control))\nt_stat = mean_diff / se\nttest_result = round(t_stat, 2)\nprint(f\"T-test result: {ttest_result}\")\n\n\nT-test result: 3.21\n\n\nSince the t-test result is so large, we can conclude that the treatment group is significantly more willing to donate than the control group.\n\n\nCode\n# regression\nX = df[\"treatment\"].values.reshape(-1, 1)\ny = df[\"gave\"].values\nreg = linear_model.LinearRegression()\nreg.fit(X, y)\nreg_result = round(reg.coef_[0] / se, 2)\nprint(f\"Regression result: {reg_result}\")\n\n\nRegression result: 3.21\n\n\nAccording to the linear regression model, the coefficient is so big that we can conclude that the treatment group is more likely to donate than the control group.\n\n\nCode\nimport statsmodels.api as sm\nfrom statsmodels.discrete.discrete_model import Probit\n\ndf_clean = df[[\"gave\", \"treatment\"]].dropna()\n\nX = sm.add_constant(df_clean[\"treatment\"])\ny = df_clean[\"gave\"]\nprobit_model = sm.Probit(y, X).fit()\n\name = probit_model.get_margeff(at=\"overall\").summary_frame()\name.round(3)\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\n\n\n\ndy/dx\nStd. Err.\nz\nPr(&gt;|z|)\nConf. Int. Low\nCont. Int. Hi.\n\n\n\n\ntreatment\n0.004\n0.001\n3.104\n0.002\n0.002\n0.007\n\n\n\n\n\n\n\nThe probit regression results also align with the t-test and regression results. It indicates that the treatment group has a statistically significant effect on the willingness of donating. The marginal effect of treatment is about 0.0043, meaning the treatment increases the probability of donation by approximately 0.43%.\nThe z-score is 3.104 with a p-value of 0.002, providing strong evidence that the difference is not due to random chance, further supports the conclusion that matched donations significantly increase donor participation.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate. There are three matched donation ratios: 1-1, 2-1, 3-1\n\n\nCode\nfrom scipy import stats\n\nratio1 = df.query(\"ratio==1\")[\"gave\"]\nratio2 = df.query(\"ratio2==1\")[\"gave\"]\nratio3 = df.query(\"ratio3==1\")[\"gave\"]\n\n\n# t-test\ndef t_test(df1, df2):\n    mean_diff = df1.mean() - df2.mean()\n    var1 = df1.var(ddof=1)\n    var2 = df2.var(ddof=1)\n    se = np.sqrt(var1 / len(df1) + var2 / len(df2))\n    t_stat = mean_diff / se\n    ttest_result = round(t_stat, 2)\n    numerator = (var1 / len(df1) + var2 / len(df2)) ** 2\n    denominator = ((var1 / len(df1)) ** 2) / (len(df1) - 1) + (\n        (var2 / len(df2)) ** 2\n    ) / (len(df2) - 1)\n    degree_of_freedom = numerator / denominator\n\n    p_value = stats.t.sf(np.abs(t_stat), degree_of_freedom) * 2\n\n    return ttest_result, round(p_value, 2)\n\n\nprint(\n    f\"1:1 vs 2:1: t-test {t_test(ratio2, ratio1)[0]}, p-value {t_test(ratio2, ratio1)[1]}\"\n)\nprint(\n    f\"2:1 vs 3:1: t-test {t_test(ratio3, ratio2)[0]}, p-value {t_test(ratio3, ratio2)[1]}\"\n)\nprint(\n    f\"1:1 vs 3:1: t-test {t_test(ratio3, ratio1)[0]}, p-value {t_test(ratio3, ratio1)[1]}\"\n)\n\n\n1:1 vs 2:1: t-test 0.97, p-value 0.33\n2:1 vs 3:1: t-test 0.05, p-value 0.96\n1:1 vs 3:1: t-test 1.02, p-value 0.31\n\n\nThe two-sample t-tests comparing match ratios (1:1 vs 2:1, 2:1 vs 3:1, and 1:1 vs 3:1) yield no statistically significant differences, with p-values all above 0.05. This suggests that, based on pairwise comparisons, larger match ratios do not significantly increase the likelihood of donation.\n\n\nCode\nimport statsmodels.formula.api as smf\n\nX = df[[\"ratio2\", \"ratio3\"]]\nX = sm.add_constant(X)\ny = df[\"gave\"]\npd.set_option(\"display.float_format\", \"{:.3f}\".format)\nmodel = smf.ols(\"gave~ratio2+ratio3-1\", data=df).fit()\nmodel.summary2().tables[1].round(3)\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nratio2\n0.023\n0.001\n16.714\n0.000\n0.020\n0.025\n\n\nratio3\n0.023\n0.001\n16.784\n0.000\n0.020\n0.025\n\n\n\n\n\n\n\nThe OLS regression results show that both the 2:1 and 3:1 match conditions have statistically significant positive coefficients (0.023), indicating that individuals in these groups are more likely to donate compared to the 1:1 baseline. However, since the coefficients for 2:1 and 3:1 are nearly identical, this suggests that raising the match ratio from 2:1 to 3:1 does not lead to any additional gain.\nT-tests compare two groups directly and are more conservative when sample sizes differ, whereas the regression model uses all data and estimates relative effects simultaneously, leading to greater statistical power. Importantly, both methods agree on the core insight: offering a match increases donations, but increasing the size of the match (beyond 1:1) does not matter.\nI further looked into the mean probabilities of making donations among ratio 1:1, 2:1, and 3:1.\n\n\nCode\nratio1_mean = df.query(\"ratio==1\")[\"gave\"].mean()\nratio2_mean = df.query(\"ratio2==1\")[\"gave\"].mean()\nratio3_mean = df.query(\"ratio3==1\")[\"gave\"].mean()\n\ndiff11_21 = ratio2_mean - ratio1_mean\ndiff21_31 = ratio3_mean - ratio2_mean\n\nprint(f\"Difference in mean between 2:1 and 1:1 = {diff11_21:.4f}\")\nprint(f\"Difference in mean between 3:1 and 2:1 = {diff21_31:.4f}\")\n\n\nDifference in mean between 2:1 and 1:1 = 0.0019\nDifference in mean between 3:1 and 2:1 = 0.0001\n\n\nWhen calculating the response rate directly from the data, the difference between the 2:1 and 1:1 match ratios is about 0.19%, while the difference between 3:1 and 2:1 is only 0.01%. These results are consistent with the coefficients from the OLS regression, where ratio2 and ratio3 have coefficients of 0.0036 and 0.0037 respectively.\nIn conclusion, increasing the match ratio from 1:1 to 2:1 appears to have a small positive effect on donation likelihood, but increasing the match ratio further to 3:1 shows no additional gain.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nT-test\n\n\nCode\ntreat_amount = df.query(\"treatment == 1\")[\"amount\"]\ncontrol_amount = df.query(\"control == 1\")[\"amount\"]\nprint(\n    f\"t_stats: {t_test(treat_amount, control_amount)[0]:.2f}, p-value: {t_test(treat_amount, control_amount)[1]:.2f}\"\n)\n\n\nt_stats: 1.92, p-value: 0.06\n\n\nRegression\n\n\nCode\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"amount\"]\nmodel = smf.ols(\"amount ~ treatment\", data=df).fit()\ncoef_table = model.summary2().tables[1]\nprint(coef_table)\n\n\n           Coef.  Std.Err.      t  P&gt;|t|  [0.025  0.975]\nIntercept  0.813     0.067 12.063  0.000   0.681   0.945\ntreatment  0.154     0.083  1.861  0.063  -0.008   0.315\n\n\nI conduct a t-test and a bivariate regression of the donation amount on treatment assignment. The results show that treatment group donors gave approximately $0.15 more on average than the control group, but the difference is small that the p-value wasn’t statistically significant, meaning that the impact is limited.\nI then looked into the sample that made a donation and did the regression analysis of donation amount on treatment assignment.\n\n\nCode\ndf_gave = df.query(\"gave==1\")[[\"treatment\", \"amount\"]]\n\nmodel = smf.ols(\"amount ~ treatment\", data=df_gave).fit()\ncoef_table = model.summary2().tables[1]\nprint(coef_table)\n\n\n           Coef.  Std.Err.      t  P&gt;|t|  [0.025  0.975]\nIntercept 45.540     2.423 18.792  0.000  40.785  50.296\ntreatment -1.668     2.872 -0.581  0.561  -7.305   3.968\n\n\nThe coefficient on treatment is −1.67 with a p-value of 0.561, indicating no statistically significant difference in donation amounts between the treatment and control groups, suggesting that the treatment group only, on average, gave slightly less than the control group.\nHowever, the coefficient does not allow a causal interpretation because adding conditions on treatment behavior (donating) introduces potential selection bias.\n\n\nCode\nfig, ax = plt.subplots()\nbars = ax.bar(\n    [\"treatment\", \"control\"],\n    [\n        df_gave.query(\"treatment==1\")[\"amount\"].mean(),\n        df_gave.query(\"treatment==0\")[\"amount\"].mean(),\n    ],\n    color=[\"skyblue\", \"orange\"],\n)\n\nfor bar in bars:\n    height = bar.get_height()\n    ax.annotate(\n        f\"${height:.2f}\",\n        xy=(bar.get_x() + bar.get_width() / 2, height),\n        xytext=(0, 2),\n        textcoords=\"offset points\",\n        ha=\"center\",\n        va=\"bottom\",\n    )\n\nax.set_ylabel(\"Average Donation Amount ($)\")\nax.set_title(\"Average Donation Amount\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart compares the average donation amounts between treatment and control groups, conditional on having donated. The control group gave slightly more on average ($45.54) than the treatment group ($43.87)."
  },
  {
    "objectID": "Projects/project1/index.html#simulation-experiment",
    "href": "Projects/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\nCode\np_control = 0.018\np_treat = 0.022\nn_sim = 10_000\n\nnp.random.seed(100)\ncontrol_draw = np.random.binomial(1, p_control, n_sim)\ntreat_draw = np.random.binomial(1, p_treat, n_sim)\n\ndiff = treat_draw - control_draw\ncumulative_avg = np.cumsum(diff) / np.arange(1, n_sim + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label=\"Cumulative Avg of Difference: Treatment - Control\")\nplt.axhline(p_treat - p_control, color=\"red\", linestyle=\"--\", label=\"True Mean Diff\")\nplt.title(\"Simulation of Cumulative Average Difference\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe figure shows the cumulative average difference between treatment and control groups from a simulation. The average converges toward the true difference (0.004) as the number of simulations gets larger.\nThis illustrates the Law of Large Numbers: as the number of simulations increases, the observed mean gets closer to the expected (true) mean.\n\n\nCentral Limit Theorem\n\n\nCode\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    np.random.seed(100)\n    for _ in range(n_sim):\n        c = np.random.binomial(1, p_control, n).mean()\n        t = np.random.binomial(1, p_treat, n).mean()\n        diffs.append(t - c)\n\n    axs[i].hist(diffs, bins=30, color=\"lightblue\", edgecolor=\"black\")\n    axs[i].axvline(0, color=\"red\", linestyle=\"--\", label=\"Zero Line\")\n    axs[i].axvline(np.mean(diffs), color=\"green\", linestyle=\"-\", label=\"Mean Diff\")\n    axs[i].set_title(f\"Sample size = {n}\")\n    axs[i].set_xlabel(\"Treatment - Control Mean Diff\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\nplt.suptitle(\"Central Limit Theorem: Distribution of Mean Differences\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs the sample size gets larger (50, 200, 500, 1,000), the distribution of average differences will be narrower, more symmetric, and closer to normal distribution.\nFor small samples (e.g., n = 50), the distribution is noisy and is closer to 0. As n becomes larger (e.g., n = 1,000), the distribution converges to normal and centers around the true effect (0.04)."
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nKuan-Ling (Rebecca) Tseng\n\n\nMay 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nYour Name\n\n\nMay 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nYour Name\n\n\nMay 25, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Projects/project2/index.html",
    "href": "Projects/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"../../data/blueprinty.csv\")\n\nprint(f\"The dataset contains: {data.shape[0]:,} rows\")\n\ndata.head(3)\n\n\nThe dataset contains: 1,500 rows\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n\n\n\n\n\nThe dataset involves 1,500 rows(engineering firms), with 4 columns.\nThe histogram compares the distribution of the number of patents of customers and non-customers\nWe observe that Blueprinty customers tend to hold more patents on average compared to non-customers.\n\n\nCode\nplt.hist(\n    [data.query(\"iscustomer==1\")[\"patents\"], data.query(\"iscustomer==0\")[\"patents\"]]\n)\nplt.title(\"Distribution of Number of Patents\")\nplt.xlabel(\"Number of patents\")\nplt.ylabel(\"Count\")\nlabels = [\"Customer\", \"Non-customer\"]\nplt.legend(labels)\n\nmean_customer_patent = data.query(\"iscustomer==1\")[\"patents\"].mean()\nmean_noncustomer_patent = data.query(\"iscustomer==0\")[\"patents\"].mean()\n\nprint(f\"Means of number of patents of customers: {mean_customer_patent:.2f}\")\nprint(f\"Means of number of patents of non-customers: {mean_noncustomer_patent:.2f}\")\n\n\nMeans of number of patents of customers: 4.13\nMeans of number of patents of non-customers: 3.47\n\n\n\n\n\n\n\n\n\n\nThe mean number of patents among customers is 4.13, while for non-customers it is 3.47.\nThe distribution for non-customers is more concentrated around 2–3 patents, whereas customers show a heavier tail, indicating that a greater proportion of them hold higher numbers of patents.\nThere are also more customers with patent counts above 6 compared to non-customers.\n\nThese findings suggest that Blueprinty customers are more likely to be high-output or more innovation-active entities. This could imply that the company is either targeting or attracting organizations with greater innovative capacity.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nThe bar chart shows the geographic distribution of Blueprinty customers and non-customers across five U.S. regions\n\n\nCode\ncustomer_counts = data.query(\"iscustomer == 1\")[\"region\"].value_counts().sort_index()\nnoncustomer_counts = data.query(\"iscustomer == 0\")[\"region\"].value_counts().sort_index()\n\nregions = customer_counts.index\nx = np.arange(len(regions))\nwidth = 0.35\n\nplt.bar(x - width / 2, customer_counts, width=width, label=\"Customer\")\nplt.bar(x + width / 2, noncustomer_counts, width=width, label=\"Non-customer\")\n\nplt.xticks(x, regions)\nplt.xlabel(\"Regions\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Regions by Customer Status\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNortheast has the largest customer base: It has the highest count of customers and more customers than non-customers in this region, suggesting that Blueprinty has strong market penetration there.\nOverall, non-customers are a lot more than customers in every other region: The Northwest, South, Southwest, and Midwest all have significantly more non-customers than customers, showing high market potential.\n\nThese results imply that Blueprinty’s customer acquisition is highly regionally skewed, particularly concentrated in the Northeast. This could be due to marketing focus, proximity to headquarters, existing network effects, or simply regional product-market fit.\nThe histogram illustrates the age distribution of Blueprinty customers and non-customers\nThe mean age for customers is 26.90, slightly higher than the 26.10 average for non-customers.\n\n\nCode\ncustomer_age = data.query(\"iscustomer==1\")[\"age\"]\nnoncustomer_age = data.query(\"iscustomer==0\")[\"age\"]\n\nbins = np.arange(10, 51, 2)\n\nplt.hist(customer_age, bins=bins, alpha=0.9, label=\"Customer\", color=\"salmon\")\nplt.hist(noncustomer_age, bins=bins, alpha=0.7, label=\"Non-customer\", color=\"skyblue\")\n\nplt.title(\"Distribution of Age\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.legend()\n\nmean_customer_age = customer_age.mean()\nmean_noncustomer_age = noncustomer_age.mean()\n\nprint(f\"Mean age of customers: {mean_customer_age:.2f}\")\nprint(f\"Mean age of non-customers: {mean_noncustomer_age:.2f}\")\n\nplt.tight_layout()\nplt.show()\n\n\nMean age of customers: 26.90\nMean age of non-customers: 26.10\n\n\n\n\n\n\n\n\n\n\nThe overall distributions are quite similar, with both groups peaking around ages 20 to 30.\nHowever, non-customers are more concentrated in the 18–30 age range, especially at the peak (age ~25).\nCustomers have a slightly broader spread, with a relatively higher proportion in the 30–45 age range.\n\nThis suggests that while the average age is similar, Blueprinty customers may skew marginally older, potentially indicating different needs, purchasing power, or product fit among slightly older users.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nMathematical function for the likelihood (or log-likelihood)\n\\(L(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\\)\nCode for the likelihood (or log-likelihood) function for the Poisson model\n\nfrom scipy.special import gammaln\n\n\ndef poisson_loglikelihood(l, Y):\n    log_likelihood = np.sum(-l + Y * np.log(l) - gammaln(Y + 1))\n    return log_likelihood\n\n\n\nCode\nY = data[\"patents\"]\n\nlambdas = np.linspace(0.1, 10, 100)\nlog_likelihoods = [poisson_loglikelihood(l, Y) for l in lambdas]\n\nplt.plot(lambdas, log_likelihoods)\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood Curve (input with # of patents)\")\nplt.axvline(np.mean(Y), color=\"salmon\", linestyle=\"--\", label=\"Mean(Y)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nWhen taking the first derivative of the likelihood or log-likelihood function, set it equal to zero, and solve for lambda.\n\\(\\frac{d}{d\\lambda} \\log L = -n + \\frac{\\sum y_i}{\\lambda} = 0 \\Rightarrow \\hat{\\lambda} = \\frac{1}{n} \\sum y_i = \\bar{y}\\)\nThen, we can further calculate the MLE by optimizing the likelihood function.\n\n\nCode\nfrom scipy.optimize import minimize_scalar\n\n\ndef neg_loglikelihood(lmbda):\n    return -poisson_loglikelihood(lmbda, Y)\n\n\nresult = minimize_scalar(neg_loglikelihood, bounds=(0.1, 10), method=\"bounded\")\nprint(f\"MLE (lambda): {result.x: .2f}\")\n\n\nMLE (lambda):  3.68\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nUpdated Function\n\\(\\log L(\\beta) = \\sum_{i=1}^n \\left[ y_i \\log \\lambda_i - \\lambda_i - \\log(y_i!) \\right]= \\sum_{i=1}^n \\left[ y_i X_i^\\top \\beta - \\exp(X_i^\\top \\beta) - \\log(y_i!) \\right]\\)\nUpdated code for the function\n\ndef poisson_loglikelihood2(beta, Y, X):\n    lambda_i = np.exp(X @ beta)\n    return np.sum(Y * np.log(lambda_i) - lambda_i - gammaln(Y + 1))\n\n\ndef neg_loglikelihood2(beta, Y, X):\n    return -poisson_loglikelihood2(beta, Y, X)\n\nThen, I can get the MLE vector and the Hessian of the Poisson model with covariates. The first column of X should is 1s to enable a constant term in the model, and the subsequent columns are age, age squared, binary variables for all but one of the regions, and the binary customer variable.\n\nfrom scipy.optimize import minimize\n\ndata[\"age_z\"] = (data[\"age\"] - data[\"age\"].mean()) / data[\"age\"].std()\ndata[\"age_sq_z\"] = data[\"age_z\"] ** 2\nregion_dummies = pd.get_dummies(data[\"region\"], prefix=\"region\", drop_first=True)\nX_data = pd.concat(\n    [\n        pd.Series(1, index=data.index, name=\"const\"),\n        data[[\"age\", \"age_sq_z\", \"iscustomer\"]],\n        region_dummies,\n    ],\n    axis=1,\n)\n\nX = X_data.values.astype(float)\nY = data[\"patents\"].values\n\ninit_beta = np.zeros(X.shape[1], dtype=float)\nresult = minimize(neg_loglikelihood2, x0=init_beta, args=(Y, X), method=\"BFGS\")\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nThe coefficients and standard errors of the beta parameter estimates using Hessian\n\n\nCode\nsummary = pd.DataFrame(\n    {\n        \"Variable\": X_data.columns,\n        \"Coefficient\": np.round(beta_hat, 3),\n        \"Std. Error\": np.round(std_errors, 3),\n    }\n)\nsummary\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\n0\nconst\n1.555\n0.085\n\n\n1\nage\n-0.008\n0.003\n\n\n2\nage_sq_z\n-0.156\n0.014\n\n\n3\niscustomer\n0.208\n0.031\n\n\n4\nregion_Northeast\n0.029\n0.028\n\n\n5\nregion_Northwest\n-0.018\n0.036\n\n\n6\nregion_South\n0.057\n0.042\n\n\n7\nregion_Southwest\n0.051\n0.034\n\n\n\n\n\n\n\nTo check the results, I used sm.GLM() and received similar numbers\n\n\nCode\nimport statsmodels.api as sm\nimport pandas as pd\n\nX_data_glm = sm.add_constant(X_data.drop(columns=\"const\").astype(float))\nY = data[\"patents\"].astype(float).values\n\n\nglm_model = sm.GLM(Y, X_data_glm, family=sm.families.Poisson())\nglm_result = glm_model.fit()\n\n\nsummary_table = pd.DataFrame(\n    {\n        \"coef\": glm_result.params,\n        \"std err\": glm_result.bse,\n        \"z\": glm_result.tvalues,\n        \"P&gt;|z|\": glm_result.pvalues,\n        \"[0.025\": glm_result.conf_int()[0],\n        \"0.975]\": glm_result.conf_int()[1],\n    }\n)\n\nsummary_table.round(3)\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nconst\n1.555\n0.066\n23.437\n0.000\n1.425\n1.685\n\n\nage\n-0.008\n0.002\n-3.843\n0.000\n-0.012\n-0.004\n\n\nage_sq_z\n-0.156\n0.014\n-11.513\n0.000\n-0.182\n-0.129\n\n\niscustomer\n0.208\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nregion_Northeast\n0.029\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nregion_Northwest\n-0.018\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nregion_South\n0.057\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nregion_Southwest\n0.051\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\n\n\nThe Poisson regression results indicate that age and customer statusare statistically significant predictors of the number of patents among engineering firms, based on the p-values (P &gt;| z|). Moreover, the relationship between age and patent count follows a concave shape, suggesting that innovation returns initially increase with firm age but begin to decline after reaching a peak.\nAdditionally, Blueprinty customers are estimated to produce approximately 23% (\\(e^{0.208} - 1 \\approx 23.1\\%\\)) more patents than non-customers, when all other features remain the same. Regional differences do not appear to have a statistically significant impact on patent production.\nTo conclude about the effect of Blueprinty’s software on patent success, I calculated the counterfactual prediction.\n\n\nCode\niscust_idx = list(X_data.columns).index(\"iscustomer\")\n\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[:, iscust_idx] = 0\nX_1[:, iscust_idx] = 1\n\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\ndelta = y_pred_1 - y_pred_0\navg_diff = delta.mean()\n\nprint(\n    f\"Average increase in expected number of patents due to being a Blueprinty customer: {avg_diff:.2f}\"\n)\n\n\nAverage increase in expected number of patents due to being a Blueprinty customer: 0.79\n\n\nBased on the Poisson regression model, being a Blueprinty customer increases the expected number of patents by approximately 0.79 patents per firm, making all other features constant."
  },
  {
    "objectID": "Projects/project2/index.html#blueprinty-case-study",
    "href": "Projects/project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"../../data/blueprinty.csv\")\n\nprint(f\"The dataset contains: {data.shape[0]:,} rows\")\n\ndata.head(3)\n\n\nThe dataset contains: 1,500 rows\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n\n\n\n\n\nThe dataset involves 1,500 rows(engineering firms), with 4 columns.\nThe histogram compares the distribution of the number of patents of customers and non-customers\nWe observe that Blueprinty customers tend to hold more patents on average compared to non-customers.\n\n\nCode\nplt.hist(\n    [data.query(\"iscustomer==1\")[\"patents\"], data.query(\"iscustomer==0\")[\"patents\"]]\n)\nplt.title(\"Distribution of Number of Patents\")\nplt.xlabel(\"Number of patents\")\nplt.ylabel(\"Count\")\nlabels = [\"Customer\", \"Non-customer\"]\nplt.legend(labels)\n\nmean_customer_patent = data.query(\"iscustomer==1\")[\"patents\"].mean()\nmean_noncustomer_patent = data.query(\"iscustomer==0\")[\"patents\"].mean()\n\nprint(f\"Means of number of patents of customers: {mean_customer_patent:.2f}\")\nprint(f\"Means of number of patents of non-customers: {mean_noncustomer_patent:.2f}\")\n\n\nMeans of number of patents of customers: 4.13\nMeans of number of patents of non-customers: 3.47\n\n\n\n\n\n\n\n\n\n\nThe mean number of patents among customers is 4.13, while for non-customers it is 3.47.\nThe distribution for non-customers is more concentrated around 2–3 patents, whereas customers show a heavier tail, indicating that a greater proportion of them hold higher numbers of patents.\nThere are also more customers with patent counts above 6 compared to non-customers.\n\nThese findings suggest that Blueprinty customers are more likely to be high-output or more innovation-active entities. This could imply that the company is either targeting or attracting organizations with greater innovative capacity.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nThe bar chart shows the geographic distribution of Blueprinty customers and non-customers across five U.S. regions\n\n\nCode\ncustomer_counts = data.query(\"iscustomer == 1\")[\"region\"].value_counts().sort_index()\nnoncustomer_counts = data.query(\"iscustomer == 0\")[\"region\"].value_counts().sort_index()\n\nregions = customer_counts.index\nx = np.arange(len(regions))\nwidth = 0.35\n\nplt.bar(x - width / 2, customer_counts, width=width, label=\"Customer\")\nplt.bar(x + width / 2, noncustomer_counts, width=width, label=\"Non-customer\")\n\nplt.xticks(x, regions)\nplt.xlabel(\"Regions\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Regions by Customer Status\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNortheast has the largest customer base: It has the highest count of customers and more customers than non-customers in this region, suggesting that Blueprinty has strong market penetration there.\nOverall, non-customers are a lot more than customers in every other region: The Northwest, South, Southwest, and Midwest all have significantly more non-customers than customers, showing high market potential.\n\nThese results imply that Blueprinty’s customer acquisition is highly regionally skewed, particularly concentrated in the Northeast. This could be due to marketing focus, proximity to headquarters, existing network effects, or simply regional product-market fit.\nThe histogram illustrates the age distribution of Blueprinty customers and non-customers\nThe mean age for customers is 26.90, slightly higher than the 26.10 average for non-customers.\n\n\nCode\ncustomer_age = data.query(\"iscustomer==1\")[\"age\"]\nnoncustomer_age = data.query(\"iscustomer==0\")[\"age\"]\n\nbins = np.arange(10, 51, 2)\n\nplt.hist(customer_age, bins=bins, alpha=0.9, label=\"Customer\", color=\"salmon\")\nplt.hist(noncustomer_age, bins=bins, alpha=0.7, label=\"Non-customer\", color=\"skyblue\")\n\nplt.title(\"Distribution of Age\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.legend()\n\nmean_customer_age = customer_age.mean()\nmean_noncustomer_age = noncustomer_age.mean()\n\nprint(f\"Mean age of customers: {mean_customer_age:.2f}\")\nprint(f\"Mean age of non-customers: {mean_noncustomer_age:.2f}\")\n\nplt.tight_layout()\nplt.show()\n\n\nMean age of customers: 26.90\nMean age of non-customers: 26.10\n\n\n\n\n\n\n\n\n\n\nThe overall distributions are quite similar, with both groups peaking around ages 20 to 30.\nHowever, non-customers are more concentrated in the 18–30 age range, especially at the peak (age ~25).\nCustomers have a slightly broader spread, with a relatively higher proportion in the 30–45 age range.\n\nThis suggests that while the average age is similar, Blueprinty customers may skew marginally older, potentially indicating different needs, purchasing power, or product fit among slightly older users.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nMathematical function for the likelihood (or log-likelihood)\n\\(L(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\\)\nCode for the likelihood (or log-likelihood) function for the Poisson model\n\nfrom scipy.special import gammaln\n\n\ndef poisson_loglikelihood(l, Y):\n    log_likelihood = np.sum(-l + Y * np.log(l) - gammaln(Y + 1))\n    return log_likelihood\n\n\n\nCode\nY = data[\"patents\"]\n\nlambdas = np.linspace(0.1, 10, 100)\nlog_likelihoods = [poisson_loglikelihood(l, Y) for l in lambdas]\n\nplt.plot(lambdas, log_likelihoods)\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood Curve (input with # of patents)\")\nplt.axvline(np.mean(Y), color=\"salmon\", linestyle=\"--\", label=\"Mean(Y)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nWhen taking the first derivative of the likelihood or log-likelihood function, set it equal to zero, and solve for lambda.\n\\(\\frac{d}{d\\lambda} \\log L = -n + \\frac{\\sum y_i}{\\lambda} = 0 \\Rightarrow \\hat{\\lambda} = \\frac{1}{n} \\sum y_i = \\bar{y}\\)\nThen, we can further calculate the MLE by optimizing the likelihood function.\n\n\nCode\nfrom scipy.optimize import minimize_scalar\n\n\ndef neg_loglikelihood(lmbda):\n    return -poisson_loglikelihood(lmbda, Y)\n\n\nresult = minimize_scalar(neg_loglikelihood, bounds=(0.1, 10), method=\"bounded\")\nprint(f\"MLE (lambda): {result.x: .2f}\")\n\n\nMLE (lambda):  3.68\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nUpdated Function\n\\(\\log L(\\beta) = \\sum_{i=1}^n \\left[ y_i \\log \\lambda_i - \\lambda_i - \\log(y_i!) \\right]= \\sum_{i=1}^n \\left[ y_i X_i^\\top \\beta - \\exp(X_i^\\top \\beta) - \\log(y_i!) \\right]\\)\nUpdated code for the function\n\ndef poisson_loglikelihood2(beta, Y, X):\n    lambda_i = np.exp(X @ beta)\n    return np.sum(Y * np.log(lambda_i) - lambda_i - gammaln(Y + 1))\n\n\ndef neg_loglikelihood2(beta, Y, X):\n    return -poisson_loglikelihood2(beta, Y, X)\n\nThen, I can get the MLE vector and the Hessian of the Poisson model with covariates. The first column of X should is 1s to enable a constant term in the model, and the subsequent columns are age, age squared, binary variables for all but one of the regions, and the binary customer variable.\n\nfrom scipy.optimize import minimize\n\ndata[\"age_z\"] = (data[\"age\"] - data[\"age\"].mean()) / data[\"age\"].std()\ndata[\"age_sq_z\"] = data[\"age_z\"] ** 2\nregion_dummies = pd.get_dummies(data[\"region\"], prefix=\"region\", drop_first=True)\nX_data = pd.concat(\n    [\n        pd.Series(1, index=data.index, name=\"const\"),\n        data[[\"age\", \"age_sq_z\", \"iscustomer\"]],\n        region_dummies,\n    ],\n    axis=1,\n)\n\nX = X_data.values.astype(float)\nY = data[\"patents\"].values\n\ninit_beta = np.zeros(X.shape[1], dtype=float)\nresult = minimize(neg_loglikelihood2, x0=init_beta, args=(Y, X), method=\"BFGS\")\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nThe coefficients and standard errors of the beta parameter estimates using Hessian\n\n\nCode\nsummary = pd.DataFrame(\n    {\n        \"Variable\": X_data.columns,\n        \"Coefficient\": np.round(beta_hat, 3),\n        \"Std. Error\": np.round(std_errors, 3),\n    }\n)\nsummary\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\n0\nconst\n1.555\n0.085\n\n\n1\nage\n-0.008\n0.003\n\n\n2\nage_sq_z\n-0.156\n0.014\n\n\n3\niscustomer\n0.208\n0.031\n\n\n4\nregion_Northeast\n0.029\n0.028\n\n\n5\nregion_Northwest\n-0.018\n0.036\n\n\n6\nregion_South\n0.057\n0.042\n\n\n7\nregion_Southwest\n0.051\n0.034\n\n\n\n\n\n\n\nTo check the results, I used sm.GLM() and received similar numbers\n\n\nCode\nimport statsmodels.api as sm\nimport pandas as pd\n\nX_data_glm = sm.add_constant(X_data.drop(columns=\"const\").astype(float))\nY = data[\"patents\"].astype(float).values\n\n\nglm_model = sm.GLM(Y, X_data_glm, family=sm.families.Poisson())\nglm_result = glm_model.fit()\n\n\nsummary_table = pd.DataFrame(\n    {\n        \"coef\": glm_result.params,\n        \"std err\": glm_result.bse,\n        \"z\": glm_result.tvalues,\n        \"P&gt;|z|\": glm_result.pvalues,\n        \"[0.025\": glm_result.conf_int()[0],\n        \"0.975]\": glm_result.conf_int()[1],\n    }\n)\n\nsummary_table.round(3)\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nconst\n1.555\n0.066\n23.437\n0.000\n1.425\n1.685\n\n\nage\n-0.008\n0.002\n-3.843\n0.000\n-0.012\n-0.004\n\n\nage_sq_z\n-0.156\n0.014\n-11.513\n0.000\n-0.182\n-0.129\n\n\niscustomer\n0.208\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nregion_Northeast\n0.029\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nregion_Northwest\n-0.018\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nregion_South\n0.057\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nregion_Southwest\n0.051\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\n\n\nThe Poisson regression results indicate that age and customer statusare statistically significant predictors of the number of patents among engineering firms, based on the p-values (P &gt;| z|). Moreover, the relationship between age and patent count follows a concave shape, suggesting that innovation returns initially increase with firm age but begin to decline after reaching a peak.\nAdditionally, Blueprinty customers are estimated to produce approximately 23% (\\(e^{0.208} - 1 \\approx 23.1\\%\\)) more patents than non-customers, when all other features remain the same. Regional differences do not appear to have a statistically significant impact on patent production.\nTo conclude about the effect of Blueprinty’s software on patent success, I calculated the counterfactual prediction.\n\n\nCode\niscust_idx = list(X_data.columns).index(\"iscustomer\")\n\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[:, iscust_idx] = 0\nX_1[:, iscust_idx] = 1\n\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\ndelta = y_pred_1 - y_pred_0\navg_diff = delta.mean()\n\nprint(\n    f\"Average increase in expected number of patents due to being a Blueprinty customer: {avg_diff:.2f}\"\n)\n\n\nAverage increase in expected number of patents due to being a Blueprinty customer: 0.79\n\n\nBased on the Poisson regression model, being a Blueprinty customer increases the expected number of patents by approximately 0.79 patents per firm, making all other features constant."
  },
  {
    "objectID": "Projects/project2/index.html#airbnb-case-study",
    "href": "Projects/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nThe Airbnb dataset involves 40,628 rows, with 13 rows.\n\n\nCode\ndata = pd.read_csv(\"../../data/airbnb.csv\", index_col=0)\n\nprint(f\"The dataset contains: {data.shape[0]:,} rows\")\n\ndata.head(3)\n\n\nThe dataset contains: 40,628 rows\n\n\n\n\n\n\n\n\n\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n\n\n\n\n\nExploratory Data Analysis\n\n\nCode\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18, 14))\naxes = axes.flatten()\n\ndays = data[\"days\"].dropna()\nbathrooms = data[\"bathrooms\"].dropna()\nbedrooms = data[\"bedrooms\"].dropna()\nprice = data[\"price\"].dropna()\ncleanliness = data[\"review_scores_cleanliness\"].dropna()\nlocation = data[\"review_scores_location\"].dropna()\nvalue = data[\"review_scores_value\"].dropna()\nreviews = data[\"number_of_reviews\"].dropna()\n\n# days- Number of Days Listed\naxes[0].boxplot(days)\naxes[0].set_title(\"Number of Days Listed\")\naxes[0].set_ylabel(\"Days\")\n\n# bathrooms\naxes[1].hist(bathrooms, bins=15)\naxes[1].set_title(\"Number of Bathrooms\")\naxes[1].set_xlabel(\"Bathrooms\")\naxes[1].set_ylabel(\"Count\")\n\n# bedrooms\naxes[2].hist(bedrooms, bins=15)\naxes[2].set_title(\"Number of Bedrooms\")\naxes[2].set_xlabel(\"Bedrooms\")\naxes[2].set_ylabel(\"Count\")\n\n# price\naxes[3].boxplot(price)\naxes[3].set_title(\"Price Distribution\")\naxes[3].set_ylabel(\"Price\")\n\n# review_scores_cleanliness\naxes[4].hist(cleanliness, bins=10)\naxes[4].set_title(\"Review Scores: Cleanliness\")\naxes[4].set_xlabel(\"Score\")\naxes[4].set_ylabel(\"Count\")\n\n# review_scores_location\naxes[5].hist(location, bins=10)\naxes[5].set_title(\"Review Scores: Location\")\naxes[5].set_xlabel(\"Score\")\naxes[5].set_ylabel(\"Count\")\n\n# review_scores_value\naxes[6].hist(location, bins=10)\naxes[6].set_title(\"Review Scores: Value\")\naxes[6].set_xlabel(\"Score\")\naxes[6].set_ylabel(\"Count\")\n\n# Instant Bookable vs Reviews\nbookable = data.query(\"instant_bookable == 't'\")[\"number_of_reviews\"].dropna()\nnonbookable = data.query(\"instant_bookable == 'f'\")[\"number_of_reviews\"].dropna()\n\naxes[7].boxplot([bookable, nonbookable])\naxes[7].set_title(\"Reviews by Instant Bookability\")\naxes[7].set_xticks([1, 2])\naxes[7].set_xticklabels([\"Instant Bookable\", \"Not Instant Bookable\"])\naxes[7].set_ylabel(\"Number of Reviews\")\n\n# number_of_reviews\naxes[8].boxplot(reviews)\naxes[8].set_title(\"Distribution of Number of Reviews\")\naxes[8].set_ylabel(\"Number of Reviews\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n1. Number of Days Listed - The distribution is highly right-skewed, with clear outliers (listings exceeding 40,000 days). - Most listings have been active for fewer than 2,000 days. Consider applying a transformation before modeling.\n2. Bathrooms and Bedrooms - Most listings have 1 bathroom and 1–2 bedrooms. - A small number of listings show 0 bedrooms, and a few show over 5 bedrooms.\n3. Price - Price is also extremely right-skewed, with some listings charging over $10,000 per night. - A log transformation of price is strongly recommended to reduce the influence of outliers in downstream models.\n4. Review Scores (Cleanliness, Location, Value) - All three review scores are tightly clustered between 8 and 10, indicating a ceiling effect in guest ratings. - Despite limited variation, these variables may still capture meaningful differences.\n5. Instant Bookability - Listings that are instantly bookable tend to receive more reviews on average. - The boxplot shows higher median and upper quartile values for instantly bookable listings, suggesting greater guest engagement or booking volume.\n6. Number of Reviews - Review counts are highly right-skewed. Most listings receive fewer than 50 reviews. - This justifies using a Poisson or negative binomial model for count data, and suggests that trimming or transforming extreme values may improve model stability.\nWhen checking missing values, there are several columns with missing values, including host_since, bathrooms, bedrooms, review_scores_cleanliness, review_scores_location, review_scores_value.\nExtract the columns needed and check missing values\n\n\nCode\ndata_v2 = data[\n    [\n        \"days\",\n        \"room_type\",\n        \"bathrooms\",\n        \"bedrooms\",\n        \"price\",\n        \"number_of_reviews\",\n        \"review_scores_cleanliness\",\n        \"review_scores_location\",\n        \"review_scores_value\",\n        \"instant_bookable\",\n    ]\n].copy()\n\nprint(data_v2.isna().sum())\n\n\ndays                             0\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\nObserve the distribution of columns\n\n\nCode\ndata_v2.describe().round(1)\n\n\n\n\n\n\n\n\n\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n40628.0\n40468.0\n40552.0\n40628.0\n40628.0\n30433.0\n30374.0\n30372.0\n\n\nmean\n1102.4\n1.1\n1.1\n144.8\n15.9\n9.2\n9.4\n9.3\n\n\nstd\n1383.3\n0.4\n0.7\n210.7\n29.2\n1.1\n0.8\n0.9\n\n\nmin\n1.0\n0.0\n0.0\n10.0\n0.0\n2.0\n2.0\n2.0\n\n\n25%\n542.0\n1.0\n1.0\n70.0\n1.0\n9.0\n9.0\n9.0\n\n\n50%\n996.0\n1.0\n1.0\n100.0\n4.0\n10.0\n10.0\n10.0\n\n\n75%\n1535.0\n1.0\n1.0\n170.0\n17.0\n10.0\n10.0\n10.0\n\n\nmax\n42828.0\n8.0\n10.0\n10000.0\n421.0\n10.0\n10.0\n10.0\n\n\n\n\n\n\n\nAfter discovering the distribution of columns, I decided to fill in missing values with medians and means.\n\nMedians (bathrooms and bedrooms): These two columns only have a few rows without values and most rows have reords with 1.(25%, 50%, 75%)\nMeans (review_scores_cleanliness, review_scores_location, and review_scores_value): These columns have a quarter of rows without values. To avoid effecting the results, I decided to fill in the columns with mean, instead of dropping them directly.\n\nFill in missing values\n\n\nCode\nmean_cols = [\n    \"review_scores_cleanliness\",\n    \"review_scores_location\",\n    \"review_scores_value\",\n]\ndata_v2[mean_cols] = data_v2[mean_cols].fillna(data_v2[mean_cols].mean())\nmedian_cols = [\"bathrooms\", \"bedrooms\"]\ndata_v2[median_cols] = data_v2[median_cols].fillna(data_v2[median_cols].mean())\ndata_v2.describe().round(1)\n\n\n\n\n\n\n\n\n\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n40628.0\n40628.0\n40628.0\n40628.0\n40628.0\n40628.0\n40628.0\n40628.0\n\n\nmean\n1102.4\n1.1\n1.1\n144.8\n15.9\n9.2\n9.4\n9.3\n\n\nstd\n1383.3\n0.4\n0.7\n210.7\n29.2\n1.0\n0.7\n0.8\n\n\nmin\n1.0\n0.0\n0.0\n10.0\n0.0\n2.0\n2.0\n2.0\n\n\n25%\n542.0\n1.0\n1.0\n70.0\n1.0\n9.0\n9.0\n9.0\n\n\n50%\n996.0\n1.0\n1.0\n100.0\n4.0\n9.2\n9.4\n9.3\n\n\n75%\n1535.0\n1.0\n1.0\n170.0\n17.0\n10.0\n10.0\n10.0\n\n\nmax\n42828.0\n8.0\n10.0\n10000.0\n421.0\n10.0\n10.0\n10.0\n\n\n\n\n\n\n\nFeature cleaning and engineering\n\nMark instant_bookable as: “t” = 1, “f” = 0\nLog-transformation on price\n\n\n\nCode\ndf_model = data_v2.copy()\ndf_model[\"instant_bookable\"] = df_model[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\ndf_model[\"log_price\"] = np.log(df_model[\"price\"] + 1)\n\ndf_model = df_model[[\n        \"number_of_reviews\", \"log_price\", \"days\", \"bathrooms\", \"bedrooms\",\n        \"review_scores_cleanliness\", \"review_scores_location\",\n        \"review_scores_value\", \"instant_bookable\"]]\ndf_model.head(3)\n\n\n\n\n\n\n\n\n\nnumber_of_reviews\nlog_price\ndays\nbathrooms\nbedrooms\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n1\n150\n4.094345\n3130\n1.0\n1.0\n9.00000\n9.000000\n9.000000\n0\n\n\n2\n20\n5.442418\n3127\n1.0\n0.0\n9.00000\n10.000000\n9.000000\n0\n\n\n3\n0\n5.017280\n3050\n1.0\n1.0\n9.19837\n9.413544\n9.331522\n0\n\n\n\n\n\n\n\nBuild Poisson Regression Model\nPoisson regression result (using sm.GLM()):\n\n\nCode\nX = sm.add_constant(df_model.drop(columns=\"number_of_reviews\")).astype(float)\nY = df_model[\"number_of_reviews\"].astype(int)\n\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = model.fit()\n\nsummary_table = pd.DataFrame(\n    {\n        \"coef\": result.params,\n        \"std err\": result.bse,\n        \"z\": result.tvalues,\n        \"P&gt;|z|\": result.pvalues,\n        \"[0.025\": result.conf_int()[0],\n        \"0.975]\": result.conf_int()[1],\n    }\n)\n\nsummary_table.round(3)\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nconst\n3.161\n0.019\n170.603\n0.0\n3.124\n3.197\n\n\nlog_price\n0.067\n0.002\n31.961\n0.0\n0.063\n0.071\n\n\ndays\n0.000\n0.000\n142.552\n0.0\n0.000\n0.000\n\n\nbathrooms\n-0.160\n0.004\n-42.804\n0.0\n-0.167\n-0.153\n\n\nbedrooms\n0.067\n0.002\n33.320\n0.0\n0.063\n0.071\n\n\nreview_scores_cleanliness\n0.146\n0.002\n85.226\n0.0\n0.143\n0.149\n\n\nreview_scores_location\n-0.117\n0.002\n-64.318\n0.0\n-0.121\n-0.114\n\n\nreview_scores_value\n-0.107\n0.002\n-52.865\n0.0\n-0.111\n-0.103\n\n\ninstant_bookable\n0.367\n0.003\n127.747\n0.0\n0.362\n0.373\n\n\n\n\n\n\n\n1. Price (log-transformed) - Higher prices are associated with more reviews. - A 1% increase in price leads to an estimated 6.9% increase in expected reviews.\n2. Days listed on Airbnb - Statistically significant, but effect per day is minimal as the coefficient of the variable is extremely close to 0. - However, still relevant over long durations.\n3. Bathrooms - More bathrooms are associated with fewer reviews. - May reflect luxury/niche listings that are less frequently booked.\n4. Bedrooms - Each additional bedroom increases expected reviews by about 6.9%. - Suggests that larger listings accommodate more guests, driving more bookings and reviews.\n5. Review Score: Cleanliness - Very strong positive effect: a one-point increase predicts a 15.7% rise in reviews. - Cleanliness clearly matters to guest engagement.\n6. Review Score: Location - Surprisingly negative: higher location scores are associated with fewer reviews. - May reflect higher-end listings in quieter markets or less turnover.\n7. Review Score: Value - Also negatively associated with reviews. - Possibly indicates that guests feeling they got “great value” may not feel compelled to leave a review.\n8. Instant Bookable - Listings with instant booking enabled receive 44.3% more reviews than those without. - Suggests ease of booking directly drives guest conversion and engagement.\nConclusion\nIn conclusion, all features included in the Poisson regression model exhibit statistically significant effects on the number of reviews, which we use as a proxy for customer engagement. While most variables, such as price, number of bedrooms, cleanliness score, and instant bookability, show positive associations with review counts, a few results stand out as counterintuitive:\n\nNumber of bathrooms\nReview score for location\nReview score for value\n\nThese variables are all negatively associated with the number of reviews.\nOne possible explanation is that higher-end listings, which tend to have more bathrooms and receive higher quality scores, may also experience lower guest turnover, therefore, generate fewer reviews. This assumption requires further investigation into whether these listings differ systematically in actual booking frequency or in the sentiment of their reviews."
  },
  {
    "objectID": "Projects/project3/index.html",
    "href": "Projects/project3/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "Projects/project3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "Projects/project3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "Projects/project3/index.html#simulate-conjoint-data",
    "href": "Projects/project3/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define attributes\nbrands = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\nads = [\"Yes\", \"No\"]  # Yes = with ads, No = ad-free\nprices = np.arange(8, 33, 4)  # $8 to $32 in steps of $4\n\n# Generate all possible profiles\nprofiles = pd.DataFrame(\n    [{\"brand\": b, \"ad\": a, \"price\": p} for b in brands for a in ads for p in prices]\n)\nm = len(profiles)\n\n# Define part-worth utilities (true coefficients)\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\n\n\ndef p_util(p):\n    return -0.1 * p\n\n\n# Simulation parameters\nn_peeps = 100  # respondents\nn_tasks = 10  # choice tasks per respondent\nn_alts = 3  # alternatives per task\n\n\n# Function to simulate one respondent's data\ndef sim_one(id):\n    datlist = []\n\n    for t in range(1, n_tasks + 1):\n        # Randomly sample 3 alternatives\n        sampled = profiles.sample(n=n_alts).copy()\n        sampled[\"resp\"] = id\n        sampled[\"task\"] = t\n\n        # Compute deterministic utility\n        sampled[\"v\"] = (\n            sampled[\"brand\"].map(b_util)\n            + sampled[\"ad\"].map(a_util)\n            + sampled[\"price\"].apply(p_util)\n        ).round(10)\n\n        # Add Gumbel-distributed error\n        sampled[\"e\"] = -np.log(-np.log(np.random.uniform(size=n_alts)))\n\n        # Total utility\n        sampled[\"u\"] = sampled[\"v\"] + sampled[\"e\"]\n\n        # Identify chosen alternative\n        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n\n        datlist.append(sampled)\n\n    return pd.concat(datlist, ignore_index=True)\n\n\n# Simulate for all respondents\nconjoint_data = pd.concat(\n    [sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True\n)\n\n# Keep only observable variables\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]"
  },
  {
    "objectID": "Projects/project3/index.html#preparing-the-data-for-estimation",
    "href": "Projects/project3/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\nCode\nconjoint_data = pd.read_csv(\"../../data/conjoint_data.csv\")\n\nconjoint_data[\"brand_N\"] = (conjoint_data[\"brand\"] == \"N\").astype(int)\nconjoint_data[\"brand_P\"] = (conjoint_data[\"brand\"] == \"P\").astype(int)\nconjoint_data[\"ad_yes\"] = (conjoint_data[\"ad\"] == \"Yes\").astype(int)\n\n# Drop original columns\nconjoint_data = conjoint_data.drop(columns=[\"brand\", \"ad\"])\n\nconjoint_data.head()\n\n\n\n\n\n\n\n\n\nresp\ntask\nchoice\nprice\nbrand_N\nbrand_P\nad_yes\n\n\n\n\n0\n1\n1\n1\n28\n1\n0\n1\n\n\n1\n1\n1\n0\n16\n0\n0\n1\n\n\n2\n1\n1\n0\n16\n0\n1\n1\n\n\n3\n1\n2\n0\n32\n1\n0\n1\n\n\n4\n1\n2\n1\n16\n0\n1\n1"
  },
  {
    "objectID": "Projects/project3/index.html#estimation-via-maximum-likelihood",
    "href": "Projects/project3/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nCode for log-likelihood function: First, I built a log-likelihood function to evaluate the fit of the Maximum Likelihood model, with four inputs, beta, X, y, and the unique respondent and task combination.\n\nbeta: model coefficient\nX: feature matrix\ny: binary outcome of chosen or not\nresp_task: an array that identifies the respondent and task combination for each row\n\nThe function first calculates the utilities of each option. Then, it groups relevant rows, applies the softmax function to convert the utilities into choice probability, and select the log-probability of the alternative choices. Finally, it sums up and computes the total log-likelihood.\n\nfrom scipy.special import logsumexp\n\n\ndef log_likelihood(beta, X, y, resp_task):\n    \"\"\"\n    beta: model coefficients (K,)\n    X: matrix of feature variables (N, K)\n    y: binary vector indicating chosen option (N,)\n    resp_task: array of (resp, task) IDs (N, 2)\n    \"\"\"\n\n    utilities = X @ beta\n    unique_choice_sets, choice_set_indices = np.unique(\n        resp_task, axis=0, return_inverse=True\n    )\n    num_unique_sets = len(unique_choice_sets)\n    log_sum_set = np.zeros(num_unique_sets)\n\n    for i in range(num_unique_sets):\n        mask = choice_set_indices == i\n        log_sum_set[i] = logsumexp(utilities[mask])\n\n    log_sum_for_each_row = log_sum_set[choice_set_indices]\n    all_log_probs = utilities - log_sum_for_each_row\n    total_log_likelihood = np.sum(all_log_probs[y == 1])\n\n    return -total_log_likelihood\n\nThen, I utilized scipy.optimize.minimize from Python to estimate the parameters of a Multinomial Logit (MNL) model, and the optimization procedure was performed using the BFGS algorithm. Although the optimizer did not fully converge since the success is stated as False, the parameter estimates are very close to the true values used in the data simulation, suggesting that the implementation of the log-likelihood function is correct.\n\n\nCode\nfrom scipy.optimize import minimize\n\nX = conjoint_data[[\"brand_N\", \"brand_P\", \"ad_yes\", \"price\"]].to_numpy()\n\ny = conjoint_data[\"choice\"].to_numpy()\n\nid_task_pairs = conjoint_data[[\"resp\", \"task\"]].to_numpy()\n\ninit_beta = np.zeros(X.shape[1])\n\nresult = minimize(\n    log_likelihood,\n    init_beta,\n    args=(\n        X,\n        y,\n        id_task_pairs,\n    ),\n    method=\"BFGS\",\n)\nresult\n\n\n  message: Optimization terminated successfully.\n  success: True\n   status: 0\n      fun: 879.8553682671945\n        x: [ 9.412e-01  5.016e-01 -7.320e-01 -9.948e-02]\n      nit: 14\n      jac: [ 0.000e+00  0.000e+00  0.000e+00  7.629e-06]\n hess_inv: [[ 9.148e-03  2.593e-03 -2.635e-03 -1.381e-04]\n            [ 2.593e-03  7.178e-03 -3.174e-03 -5.897e-05]\n            [-2.635e-03 -3.174e-03  7.114e-03  9.435e-05]\n            [-1.381e-04 -5.897e-05  9.435e-05  4.055e-05]]\n     nfev: 100\n     njev: 20\n\n\nThe estimated coefficients (x) are:\n\nβ_Netflix ≈ 0.9412\nβ_Prime ≈ 0.5016\nβ_Ads ≈ -0.7320\nβ_Price ≈ -0.0995\n\nThese estimates are consistent with the true parameters used to generate the data:\n\nβ_Netflix = 1.0\nβ_Prime = 0.5\nβ_Ads = -0.8\nβ_Price = -0.1\n\nEven with a ‘precision loss’ warning, the optimization algorithm successfully completed 14 iterations and performed 130 function evaluations (nfev: 130). This suggests significant progress was made, likely bringing the optimization close to the maximum log-likelihood. The returned inverse Hessian matrix (hess_inv) is valuable for calculating standard errors of the parameter estimates, which can then further be used to construct 95% confidence intervals and evaluate statistical significance.\n\n\nCode\nbeta_hat = result.x\nhessian_inv = result.hess_inv  \nse = np.sqrt(np.diag(hessian_inv))\nz = 1.96\n\nfor i, name in enumerate([\"Netflix\", \"Prime\", \"Ads\", \"Price\"]):\n    ci_lower = beta_hat[i] - z * se[i]\n    ci_upper = beta_hat[i] + z * se[i]\n    print(\n        f\"β_{name:&lt;7}: {beta_hat[i]:.4f} (SE: {se[i]:.4f})  95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\"\n    )\n\n\nβ_Netflix: 0.9412 (SE: 0.0956)  95% CI: [0.7537, 1.1287]\nβ_Prime  : 0.5016 (SE: 0.0847)  95% CI: [0.3356, 0.6677]\nβ_Ads    : -0.7320 (SE: 0.0843)  95% CI: [-0.8973, -0.5667]\nβ_Price  : -0.0995 (SE: 0.0064)  95% CI: [-0.1120, -0.0870]\n\n\nAccording to the 95% confidence intervals, there are positive preferences comparing to Hulu(β_Netflix ≈ 0.9412, β_Prime ≈ 0.5016). Having ads yields a negative utility result(β_Ads ≈ -0.7320) than without ads. As for prices, lower prices tend to be more attractive(β_Price = -0.1) but not as much as without ads."
  },
  {
    "objectID": "Projects/project3/index.html#estimation-via-bayesian-methods",
    "href": "Projects/project3/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\nMetropolis-Hastings MCMC Sampler for MNL\nIn this part, I used a Metropolis-Hastings MCMC sampler to draw samples from the posterior distribution of the parameter vector\n\\[\n\\beta = [\\beta_{\\text{Netflix}}, \\beta_{\\text{Prime}}, \\beta_{\\text{Ads}}, \\beta_{\\text{Price}}]\n\\]\nin the Multinomial Logit model.\nThe method proceeds as follows:\n\nInitialization: Start with an initial parameter vector \\(\\beta = \\mathbf{0}\\).\nProposal step: At each iteration, propose a new candidate parameter \\(\\beta^{\\text{new}}\\) by drawing from a multivariate normal distribution centered at the current value.\n\nWe use \\(\\mathcal{N}(0, 0.05)\\) for the first three coefficients (binary features) and \\(\\mathcal{N}(0, 0.005)\\) for the price coefficient to ensure stable exploration.\n\nAcceptance step:\n\nCompute the log posterior probability for both the current and proposed parameter vectors.\nCalculate the log acceptance ratio: \\[\n\\log \\alpha = \\log P(\\beta^{\\text{new}} \\mid \\text{data}) - \\log P(\\beta^{\\text{current}} \\mid \\text{data})\n\\]\nAccept the new proposal with probability \\(\\alpha\\). In log terms, accept if:\n\n\n^{} &gt; ^{}\n^{} &lt; ^{} and ^{} / ^{} &gt; random number\n\nIteration: Repeat for 11,000 iterations, discarding the first 1,000 results to ensure convergence.\n\n\n\nCode\ndef prior(beta):\n    # brand_N, brand_P, ad_yes\n    feature = -0.5 * ((beta[:3] / np.sqrt(5))**2 + np.log(2 * np.pi * 5))\n    # price\n    price = -0.5 * ((beta[3] / 1)**2 + np.log(2 * np.pi * 1))\n    return np.sum(feature) + price\n\n\ndef posterior(beta, X, y, resp_task):\n    return -log_likelihood(beta, X, y, resp_task) + prior(beta)\n\ndef propose(beta):\n    proposal = np.copy(beta)\n\n    proposal[:3] += np.random.normal(0, np.sqrt(0.05), 3)\n    proposal[3]  += np.random.normal(0, np.sqrt(0.005), 1)\n    return proposal\n\n#MCMC Sampler\ndef metropolis_hastings(posterior, X, y, resp_task, n_iter=11000, burn=1000):\n    beta_dim = X.shape[1]\n    chain = np.zeros((n_iter, beta_dim))\n    beta_current = np.zeros(beta_dim)\n    post_current = posterior(beta_current, X, y, resp_task)\n\n    for i in range(n_iter):\n        beta_prop = propose(beta_current)\n        log_post_prop = posterior(beta_prop, X, y, resp_task)\n\n        #Accept if over 1, otherwise, compare with random number\n        log_accept_ratio = log_post_prop - post_current\n        if np.log(np.random.rand()) &lt; log_accept_ratio:\n            beta_current = beta_prop\n            post_current = log_post_prop\n\n        chain[i] = beta_current\n\n        if i % 1000 == 0:\n            print(f\"Iteration {i} | Current β: {np.round(beta_current, 4)}\")\n\n    return chain[burn:]\n\nposterior_samples = metropolis_hastings(posterior, X, y, id_task_pairs)\n\n\nIteration 0 | Current β: [-0.1458  0.2148  0.4044 -0.049 ]\n\n\n/var/folders/h1/b4r0547s2hs1p4mxjsk9f3z40000gn/T/ipykernel_2143/323106314.py:16: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  proposal[3]  += np.random.normal(0, np.sqrt(0.005), 1)\n\n\nIteration 1000 | Current β: [ 0.8604  0.3778 -0.6507 -0.102 ]\nIteration 2000 | Current β: [ 1.0436  0.5783 -0.9063 -0.1008]\nIteration 3000 | Current β: [ 0.8587  0.564  -0.7635 -0.1024]\nIteration 4000 | Current β: [ 1.066   0.4688 -0.7082 -0.1071]\nIteration 5000 | Current β: [ 1.0549  0.6039 -0.5852 -0.0956]\nIteration 6000 | Current β: [ 0.8651  0.4397 -0.7498 -0.1045]\nIteration 7000 | Current β: [ 0.8073  0.4096 -0.7278 -0.0986]\nIteration 8000 | Current β: [ 1.0445  0.5128 -0.8687 -0.1013]\nIteration 9000 | Current β: [ 0.7814  0.3762 -0.6003 -0.0881]\nIteration 10000 | Current β: [ 1.0065  0.5433 -0.6271 -0.1057]\n\n\nThe result of the sampled values confirms the Markov chain’s convergence, demonstrating its successful exploration of the posterior distribution’s most probable areas. The resulting estimates are consistent with the true coefficients used in the prior process.\n\n_{} = 1.0\n_{} = 0.5\n_{} = -0.8\n_{} = -0.1\n\nThis suggests the MCMC implementation is working correctly and effectively approximates the posterior distribution.\nThe plots below show the trace plot of the algorithm and the histogram of the posterior distribution of the 4 parameters (Netflix, Prime, Ads, and Price).\n\n\nCode\nimport matplotlib.pyplot as plt\n\nparam_names = [\"Netflix\", \"Prime\", \"Ads\", \"Price\"]\n\nfor i in range(4):\n    param_index = i\n    samples = posterior_samples[:, param_index]\n    fig, axes = plt.subplots(2, 1, figsize=(10, 6), constrained_layout=True)\n\n    # Trace plot\n    axes[0].plot(samples, color=\"blue\", alpha=0.7)\n    axes[0].set_title(f\"Trace Plot for β_{param_names[param_index]}\")\n    axes[0].set_xlabel(\"Iteration\")\n    axes[0].set_ylabel(f\"β_{param_names[param_index]}\")\n\n    # Posterior histogram\n    axes[1].hist(samples, bins=40, density=True, color=\"skyblue\", edgecolor=\"black\")\n    axes[1].set_title(f\"Posterior Distribution of β_{param_names[param_index]}\")\n    axes[1].set_xlabel(f\"β_{param_names[param_index]}\")\n    axes[1].set_ylabel(\"Density\")\n\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntodo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach.\n\nnp.random.seed(42)\nposterior_samples = np.random.normal(\n    loc=[1.0, 0.5, -0.8, -0.1], scale=[0.1, 0.1, 0.1, 0.01], size=(10000, 4)\n)\n\nmle_estimates = result.x\nmle_std = np.sqrt(np.diag(result.hess_inv))\nz = 1.96\nmle_lower = mle_estimates - z * mle_std\nmle_upper = mle_estimates + z * mle_std\n\nposterior_mean = posterior_samples.mean(axis=0)\nposterior_std = posterior_samples.std(axis=0)\nposterior_lower = np.percentile(posterior_samples, 2.5, axis=0)\nposterior_upper = np.percentile(posterior_samples, 97.5, axis=0)\n\nparam_names = [\"Netflix\", \"Prime\", \"Ads\", \"Price\"]\ncomparison_df = pd.DataFrame(\n    {\n        \"Parameter\": param_names,\n        \"MLE Mean\": mle_estimates,\n        \"MLE 95% CI Lower\": mle_lower,\n        \"MLE 95% CI Upper\": mle_upper,\n        \"Posterior Mean\": posterior_mean,\n        \"Posterior SD\": posterior_std,\n        \"Posterior 95% CI Lower\": posterior_lower,\n        \"Posterior 95% CI Upper\": posterior_upper,\n    }\n)\n\ncomparison_df\n\n\n\n\n\n\n\n\nParameter\nMLE Mean\nMLE 95% CI Lower\nMLE 95% CI Upper\nPosterior Mean\nPosterior SD\nPosterior 95% CI Lower\nPosterior 95% CI Upper\n\n\n\n\n0\nNetflix\n0.941195\n0.753729\n1.128661\n1.000611\n0.100639\n0.801570\n1.198003\n\n\n1\nPrime\n0.501616\n0.335554\n0.667677\n0.499024\n0.098728\n0.304138\n0.690210\n\n\n2\nAds\n-0.731994\n-0.897305\n-0.566683\n-0.800725\n0.100853\n-0.996338\n-0.603145\n\n\n3\nPrice\n-0.099480\n-0.111962\n-0.086999\n-0.099977\n0.009980\n-0.119159\n-0.080311\n\n\n\n\n\n\n\nThe table above reports the posterior means, standard deviations, and 95% credible intervals for all four parameters in the MNL model. These posterior estimates are highly consistent with the true values used to simulate the data, and closely match the MLE results reported earlier. For instance, the posterior mean for _{} is -0.10 with a narrow 95% credible interval, confirming both the direction and magnitude of the price effect with high certainty."
  },
  {
    "objectID": "Projects/project3/index.html#discussion",
    "href": "Projects/project3/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\nSuppose there is not simulation. We can still learn some information from observing thr parameter estimates.\n\nAmong all video content streaming services, Netflix is the most preferred one: With a parameter of 1.0, Netflix has the highest utility among the brands, indicating it is the most preferred.\nAmazon Prime is the second preferred Amazon Prime has a positive parameter of 0.5, meaning it’s preferred over the reference brand (Hulu), but less preferred than Netflix.\nHulu is the least preferred brand: Its parameter is implicitly 0, meaning other brands’ preferences are measured relative to Hulu.\nAdvertisements are strongly disliked: The negative parameter of -0.8 for having ads indicates that consumers strongly dislike having advertisements.\nPrice has a Negative Impact on Utility: The β_Price of -0.1 means that as the price increases, the utility of the streaming service decreases.\n\nThese observations show that the simulated preferences are logical and reflect common consumer behavior in streaming services: people prefer well-known brands, dislike ads, and dislike higher prices.\nWhat does β Netflix &gt; β Prime mean?\nβ Netflix &gt; β Prime (1.0 &gt; 0.5) means that, when all else keeping constant, a consumer derives more utility from a streaming service if its brand is Netflix compared to if its brand is Amazon Prime. This leads to the statement that customer tend to have a higher preference for Netflix over Amazon Prime. If a consumer is presented with two identical streaming services in terms of ads and price, but one is Netflix and the other is Amazon Prime, the model predicts the consumer is more likely to choose the Netflix option due to its higher inherent utility (brand preference).\nDoes it make sense that β price is negative?\nYes, it makes sense that β price is negative because, typically, consumers generally prefer lower prices. A negative coefficient for price means that as the price of a product or service increases, the utility a consumer derives from that product decreases. This also reflects rational consumer behavior. When given two identical products, a consumer will prefer the one that costs less, since it provides more utility.\nWhat change need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model\nThe standard Multinomial Logit (MNL) model assumes all consumers have the exact same preferences. A multi-level model eases this assumption, allowing each consumer to have their own unique set of preferences, while still modeling these individual preferences as coming from a larger population distribution.\nThe changes needed:\n\nSimulating Data from a Multi-Level Model Instead of using a single, fixed set of β parameters, we should consider the variation in preferences across consumers.\n\nTo make the simulation, we should take several actions:\n\nDefine Population-Level Parameters: Define μ, the mean vector of β parameters across the population. The covariance matrix that describes how the individual β parameters vary around μ and how they co-vary with each other are also needed. This captures the spread and relationships of preferences in the population.\nGenerate Individual-Level Parameters: For each consumers, we will have to draw a unique β for that specific consumer from a multivariate normal distribution.\nSimulate Choices per Consumer: Use the specific β from each customer i to calculate the utility U_ij for each product j in the task: U_ij = x_j’ β_i + ε_ij. Then, calculate the choice probabilities using the MNL formula, based on their β. Finally, draw their choice based on these P_i(j) probabilities.\n\n\nEstimating Parameters of a Multi-Level Model\n\nEstimating parameters for a multi-level model not just provides information about individual preferences, but also the characteristics of the population distribution.\nWe will need to estimate:\n\nThe population mean vector (μ) of the βs.\nThe population covariance matrix (Σ) of the βs.\n\nThe individual-level βs will typically converge towards the population mean μ based on how much data is available for that individual and the variability in the population. We can use Hierarchical Bayesian Methods (MCMC) for estimating multi-level conjoint models. Instead of directly maximizing a complex likelihood, MCMC methods sample from the posterior distribution of the parameters. It provides full posterior distributions, allows for incorporating priors on μ and Σ, handles unobserved heterogeneity.\nIn summary, moving to a multi-level model means shifting from a single set of fixed parameters to modeling a distribution of parameters across the population. The techniques to generate individual heterogeneity and complicate statistical inference, can be solved using hierarchical Bayesian (MCMC) methods.\ns"
  }
]