[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kuan-Ling Tseng (Rebecca)",
    "section": "",
    "text": "Welcome!!!"
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the experiment, the authors collaborated with a nonprofit organization and conducted a large-scale field experiment involving over 50,000 prior donors. Each donor was randomly assigned to receive one of several types of direct-mail fundraising letters.\nThe control group received a standard letter with no match offer, while the treatment groups received a matching grant offer, where an anonymous donor pledged to match donations at a rate of 1:1, 2:1, or 3:1. Additionally, the letters varied in their presentation of the suggested donation amount and the total match cap ($25,000 / $50,000 / $100,000 / unstated).\nThe researchers then tracked each recipient’s response—specifically: • Whether they donated at all (gave) • How much they donated (amount)\nThis allowed the authors to examine whether matching grants increase donations, whether the size of the match ratio matters, and how the differences influence donor behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the experiment, the authors collaborated with a nonprofit organization and conducted a large-scale field experiment involving over 50,000 prior donors. Each donor was randomly assigned to receive one of several types of direct-mail fundraising letters.\nThe control group received a standard letter with no match offer, while the treatment groups received a matching grant offer, where an anonymous donor pledged to match donations at a rate of 1:1, 2:1, or 3:1. Additionally, the letters varied in their presentation of the suggested donation amount and the total match cap ($25,000 / $50,000 / $100,000 / unstated).\nThe researchers then tracked each recipient’s response—specifically: • Whether they donated at all (gave) • How much they donated (amount)\nThis allowed the authors to examine whether matching grants increase donations, whether the size of the match ratio matters, and how the differences influence donor behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\ndf = pd.read_stata(\"./data/karlan_list_2007.dta\", iterator=False)\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nfrom sklearn import linear_model\nimport numpy as np\n\ntest_variables = [\"hpa\", \"mrm2\", \"freq\", \"years\", \"year5\", \"dormant\"]\n\n\ndef compare_ttest_regress(variable):\n\n    subset = df[[\"treatment\", variable]].dropna()\n    treat = subset[subset[\"treatment\"] == 1][variable]\n    control = subset[subset[\"treatment\"] == 0][variable]\n\n    # t-test\n    mean_diff = treat.mean() - control.mean()\n    var_treat = treat.var(ddof=1)\n    var_control = control.var(ddof=1)\n    se = np.sqrt(var_treat / len(treat) + var_control / len(control))\n    t_stat = mean_diff / se\n    ttest_result = round(t_stat, 2)\n\n    # regression\n    X = subset[\"treatment\"].values.reshape(-1, 1)\n    y = subset[variable].values\n    reg = linear_model.LinearRegression()\n    reg.fit(X, y)\n    reg_result = round(reg.coef_[0] / se, 2)\n\n    return ttest_result, reg_result\n\n\nresults = []\nfor v in test_variables:\n    results.append(\n        {\n            \"variable\": v,\n            \"t_test\": compare_ttest_regress(v)[0],\n            \"regress\": compare_ttest_regress(v)[1],\n        }\n    )\n\ndf_result = pd.DataFrame(results)\ndf_result\n\n\n\n\n\n\n\n\nvariable\nt_test\nregress\n\n\n\n\n0\nhpa\n0.97\n0.97\n\n\n1\nmrm2\n0.12\n0.12\n\n\n2\nfreq\n-0.11\n-0.11\n\n\n3\nyears\n-1.09\n-1.09\n\n\n4\nyear5\n-1.56\n-1.56\n\n\n5\ndormant\n0.17\n0.17"
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport matplotlib.pyplot as plt\n\n\ndf_treat = df.query(\"treatment==1\")\ndf_control = df.query(\"control==1\")\ntreat_prop = df_treat[\"gave\"].value_counts(normalize=True)\ncontrol_prop = df_control[\"gave\"].value_counts(normalize=True)\n\nfig, ax = plt.subplots()\nbars = ax.bar(\n    [\"treat\", \"control\"],\n    [treat_prop[1] * 100, control_prop[1] * 100],\n    color=[\"skyblue\", \"orange\"],\n)\n\nfor bar in bars:\n    height = bar.get_height()\n    ax.annotate(\n        f\"{height:.1f}%\",\n        xy=(bar.get_x() + bar.get_width() / 2, height),\n        xytext=(0, 2),\n        textcoords=\"offset points\",\n        ha=\"center\",\n        va=\"bottom\",\n    )\n\n\n\n\n\n\n\n\n\ntreat = df[df[\"treatment\"] == 1][\"gave\"]\ncontrol = df[df[\"treatment\"] == 0][\"gave\"]\n# t-test\nmean_diff = treat.mean() - control.mean()\nvar_treat = treat.var(ddof=1)\nvar_control = control.var(ddof=1)\nse = np.sqrt(var_treat / len(treat) + var_control / len(control))\nt_stat = mean_diff / se\nttest_result = round(t_stat, 2)\nttest_result\n\nnp.float64(3.21)\n\n\nSince the t-test result is so large, we can conclude that the treatment group is significantly more willing to donate than the control group.\n\n# regression\nX = df[\"treatment\"].values.reshape(-1, 1)\ny = df[\"gave\"].values\nreg = linear_model.LinearRegression()\nreg.fit(X, y)\nreg_result = round(reg.coef_[0] / se, 2)\nreg_result\n\nnp.float64(3.21)\n\n\nAccording to the linear regression model, the coefficient is so big that we can conclude that the treatment group is more likely to donate than the control group.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\ndf_clean = df[[\"gave\", \"treatment\"]].dropna()\n\nX = sm.add_constant(df_clean[\"treatment\"])\ny = df_clean[\"gave\"]\nprobit_model = sm.Probit(y, X).fit()\n\nprint(probit_model.summary())\n\nmfx = probit_model.get_margeff()\nprint(mfx.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 23 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        10:38:22   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\nBased on the probit regression result, people in the treatment group are significantly more inclined to donate money.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nfrom scipy import stats\n\nratio1 = df.query(\"ratio==1\")[\"gave\"]\nratio2 = df.query(\"ratio2==1\")[\"gave\"]\nratio3 = df.query(\"ratio3==1\")[\"gave\"]\n\n\n# t-test\ndef t_test(df1, df2):\n    mean_diff = df1.mean() - df2.mean()\n    var1 = df1.var(ddof=1)\n    var2 = df2.var(ddof=1)\n    se = np.sqrt(var1 / len(df1) + var2 / len(df2))\n    t_stat = mean_diff / se\n    ttest_result = round(t_stat, 2)\n    degree_of_freedom = (var1 / len(df1) + var1 / len(df2)) ** 2 / (\n        ((var1 / len(df1)) ** 2) / (len(df1) - 1)\n        + ((var1 / len(df2)) ** 2) / (len(df2) - 1)\n    )\n\n    p_value = stats.t.sf(np.abs(t_stat), degree_of_freedom) * 2\n\n    return ttest_result, round(p_value, 2)\n\n\nprint(\n    f\"1:1 vs 2:1: t-test {t_test(ratio2, ratio1)[0]}, p-value {t_test(ratio2, ratio1)[1]}\"\n)\nprint(\n    f\"2:1 vs 3:1: t-test {t_test(ratio3, ratio2)[0]}, p-value {t_test(ratio3, ratio2)[1]}\"\n)\nprint(\n    f\"1:1 vs 3:1: t-test {t_test(ratio3, ratio1)[0]}, p-value {t_test(ratio3, ratio1)[1]}\"\n)\n\n1:1 vs 2:1: t-test 0.97, p-value 0.33\n2:1 vs 3:1: t-test 0.05, p-value 0.96\n1:1 vs 3:1: t-test 1.02, p-value 0.31\n\n\nThe results show that there are no significant difference between match ratios, 1:1 vs 2:1, 2:1 vs 3:1, or 1:1 vs 3:1.\n\nX = df[[\"ratio2\", \"ratio3\"]]\nX = sm.add_constant(X)\ny = df[\"gave\"]\n\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     4.117\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0163\nTime:                        10:38:22   Log-Likelihood:                 26629.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50080   BIC:                        -5.323e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0190      0.001     22.306      0.000       0.017       0.021\nratio2         0.0036      0.002      2.269      0.023       0.000       0.007\nratio3         0.0037      0.002      2.332      0.020       0.001       0.007\n==============================================================================\nOmnibus:                    59815.856   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4317637.927\nSkew:                           6.741   Prob(JB):                         0.00\nKurtosis:                      46.443   Cond. No.                         3.16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nratio1_mean = df.query(\"(ratio2 == 0) & (ratio3 == 0)\")[\"gave\"].mean()\nratio2_mean = df.query(\"ratio2==1\")[\"gave\"].mean()\nratio3_mean = df.query(\"ratio3==1\")[\"gave\"].mean()\n\ndiff11_21 = ratio2_mean - ratio1_mean\ndiff21_31 = ratio3_mean - ratio2_mean\n\nprint(f\"2:1 - 1:1 = {diff11_21:.4f}\")\nprint(f\"3:1 - 2:1 = {diff21_31:.4f}\")\n\n2:1 - 1:1 = 0.0036\n3:1 - 2:1 = 0.0001\n\n\nWhen calculating the response rate directly from the data, the difference between the 2:1 and 1:1 match ratios is about 0.36, while the difference between 3:1 and 2:1 is only 0.01. These results are consistent with the coefficients from the OLS regression, where ratio2 and ratio3 have coefficients of 0.0036 and 0.0037 respectively.\nIn conclusion, increasing the match ratio from 1:1 to 2:1 appears to have a small positive effect on donation likelihood, but increasing the match ratio further to 3:1 shows no additional gain.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nT-test\n\ntreat_amount = df.query(\"treatment == 1\")[\"amount\"]\ncontrol_amount = df.query(\"control == 1\")[\"amount\"]\nprint(\n    f\"t_stats: {t_test(treat_amount, control_amount)[0]:.2f}, p-value: {t_test(treat_amount, control_amount)[1]:.2f}\"\n)\n\nt_stats: 1.92, p-value: 0.06\n\n\nLinear Regression\n\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"amount\"]\nmodel = sm.OLS(y, X, missing=\"drop\").fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0628\nTime:                        10:38:22   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe conduct a t-test and a bivariate regression of the donation amount on treatment assignment. The results show that treatment group donors gave approximately $0.15 more on average than the control group, but the difference is small that the p-value wasn’t statistically significant, meaning that the impact is limited.\n\ntreat_amount_gave = df.query(\"treatment == 1 and gave==1\")[\"amount\"]\ncontrol_amount_gave = df.query(\"control == 1 and gave==1\")[\"amount\"]\nX = sm.add_constant(df.query(\"gave==1\")[\"treatment\"])\ny = df.query(\"gave==1\")[\"amount\"]\nmodel = sm.OLS(y, X, missing=\"drop\").fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.561\nTime:                        10:38:22   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe then looked into the sample that made a donation and did the regression analysis o donation amount on treatment assignment. The coefficient on treatment is −1.67 with a p-value of 0.561, indicating no statistically significant difference in donation amounts between the treatment and control groups, suggesting that the treatment group only, on average, gave slightly less than the control group.\nHowever, the coefficient does not allow a causal interpretation because adding conditions on treatment behavior (donating) introduces potential selection bias.\n\nfig, ax = plt.subplots()\nbars = ax.bar(\n    [\"treat\", \"control\"],\n    [treat_amount_gave.mean(), control_amount_gave.mean()],\n    color=[\"skyblue\", \"orange\"],\n)\n\nfor bar in bars:\n    height = bar.get_height()\n    ax.annotate(\n        f\"${height:.2f}\",\n        xy=(bar.get_x() + bar.get_width() / 2, height),\n        xytext=(0, 2),\n        textcoords=\"offset points\",\n        ha=\"center\",\n        va=\"bottom\",\n    )\n\nax.set_ylabel(\"Average Donation Amount ($)\")\nax.set_title(\"Average Donation Amount\")\n\nplt.show()\n\n\n\n\n\n\n\n\nThe bar chart compares the average donation amounts between treatment and control groups, conditional on having donated. The control group gave slightly more on average ($45.54) than the treatment group ($43.87)."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\np_control = 0.018\np_treat = 0.022\nn_sim = 10_000\n\nnp.random.seed(100)\ncontrol_draw = np.random.binomial(1, p_control, n_sim)\ntreat_draw = np.random.binomial(1, p_treat, n_sim)\n\ndiff = treat_draw - control_draw\ncumulative_avg = np.cumsum(diff) / np.arange(1, n_sim + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label=\"Cumulative Avg of Difference: Treatment - Control\")\nplt.axhline(p_treat - p_control, color=\"red\", linestyle=\"--\", label=\"True Mean Diff\")\nplt.title(\"Simulation of Cumulative Average Difference\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe figure shows the cumulative average difference between treatment and control groups from a simulation. The average converges toward the true difference (0.004) as the number of simulations gets larger.\nThis illustrates the Law of Large Numbers: as the number of simulations increases, the observed mean gets closer to the expected (true) mean.\n\n\nCentral Limit Theorem\n\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    np.random.seed(100)\n    for _ in range(n_sim):\n        c = np.random.binomial(1, p_control, n).mean()\n        t = np.random.binomial(1, p_treat, n).mean()\n        diffs.append(t - c)\n\n    axs[i].hist(diffs, bins=30, color=\"lightblue\", edgecolor=\"black\")\n    axs[i].axvline(0, color=\"red\", linestyle=\"--\", label=\"Zero Line\")\n    axs[i].axvline(np.mean(diffs), color=\"green\", linestyle=\"-\", label=\"Mean Diff\")\n    axs[i].set_title(f\"Sample size = {n}\")\n    axs[i].set_xlabel(\"Treatment - Control Mean Diff\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\nplt.suptitle(\"Central Limit Theorem: Distribution of Mean Differences\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAs the sample size gets larger (50, 200, 500, 1,000), the distribution of average differences will be narrower, more symmetric, and closer to normal distribution.\nFor small samples (e.g., n = 50), the distribution is noisy and is closer to 0. As n becomes larger (e.g., n = 1,000), the distribution converges to normal and centers around the true effect (0.04)."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "Projects/project1/index.html",
    "href": "Projects/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment collaborated with a nonprofit organization involving over 50,000 prior donors. Each donor was randomly assigned to receive one of several types of direct-mail fundraising letters.\nThe control group received a standard letter with no match offer, while the treatment groups received a matching grant offer, where an anonymous donor pledged to match donations at a rate of 1:1, 2:1, or 3:1. Additionally, the letters varied in their presentation of the suggested donation amount and the total match maximum ($25,000 / $50,000 / $100,000 / unstated).\nThe researchers then tracked each recipient’s response—specifically:\n\nWhether they donated at all (gave)\nHow much they donated (amount)\n\nThis allowed the authors to examine whether matching grants increase donations, whether the size of the match ratio matters, and how the differences influence donor behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "Projects/project1/index.html#introduction",
    "href": "Projects/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment collaborated with a nonprofit organization involving over 50,000 prior donors. Each donor was randomly assigned to receive one of several types of direct-mail fundraising letters.\nThe control group received a standard letter with no match offer, while the treatment groups received a matching grant offer, where an anonymous donor pledged to match donations at a rate of 1:1, 2:1, or 3:1. Additionally, the letters varied in their presentation of the suggested donation amount and the total match maximum ($25,000 / $50,000 / $100,000 / unstated).\nThe researchers then tracked each recipient’s response—specifically:\n\nWhether they donated at all (gave)\nHow much they donated (amount)\n\nThis allowed the authors to examine whether matching grants increase donations, whether the size of the match ratio matters, and how the differences influence donor behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "Projects/project1/index.html#data",
    "href": "Projects/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset contains 51 columns, with 50,083 rows (mails sent). There are 33,396 mails sent to the treatment group, while 16,687 mails sent to the control group. As for the match ratio, 11,133 mails got 1:1, 11,134 mails got 2:1, and 11,129 mails got 3:1.\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_stata(\n    \"../../data/karlan_list_2007.dta\",\n    iterator=False,\n)\nprint(f\"Rows: {df.shape[0]:,}, Columns: {df.shape[1]:,}\")\nprint(f\"Treatment group: {df.treatment.sum():,}, Contol group: {df.control.sum():,}\")\nprint(\n    f\"Ratio 1:1: {sum(df.ratio == 1):,}, Ratio 2:1: {sum(df.ratio2 == 1):,}, Ratio 3:1: {sum(df.ratio3 == 1):,}\"\n)\ndf.head()\n\n\nRows: 50,083, Columns: 51\nTreatment group: 33,396, Contol group: 16,687\nRatio 1:1: 11,133, Ratio 2:1: 11,134, Ratio 3:1: 11,129\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168561\n0.135868\n0.103039\n0.378115\n22027.316665\n0.193405\n0.186599\n0.258654\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nSelected Variables:hpa, mrm2, freq, years, year5, dormant\nFor the selected variables, I did T-test and regression to examine the difference between two groups - control group and treatment group.\n\n\nCode\nfrom sklearn import linear_model\nimport numpy as np\n\ntest_variables = [\"hpa\", \"mrm2\", \"freq\", \"years\", \"year5\", \"dormant\"]\n\n\ndef compare_ttest_regress(variable):\n\n    subset = df[[\"treatment\", variable]].dropna()\n    treat = subset[subset[\"treatment\"] == 1][variable]\n    control = subset[subset[\"treatment\"] == 0][variable]\n\n    # t-test\n    mean_diff = treat.mean() - control.mean()\n    var_treat = treat.var(ddof=1)\n    var_control = control.var(ddof=1)\n    se = np.sqrt(var_treat / len(treat) + var_control / len(control))\n    t_stat = mean_diff / se\n    ttest_result = round(t_stat, 2)\n\n    # regression\n    X = subset[\"treatment\"].values.reshape(-1, 1)\n    y = subset[variable].values\n    reg = linear_model.LinearRegression()\n    reg.fit(X, y)\n    reg_result = round(reg.coef_[0] / se, 2)\n\n    return treat.mean(), control.mean(), mean_diff, ttest_result, reg_result\n\n\nresults = []\nfor v in test_variables:\n    results.append(\n        {\n            \"variable\": v,\n            \"control mean\": compare_ttest_regress(v)[0],\n            \"treatment mean\": compare_ttest_regress(v)[1],\n            \"difference\": compare_ttest_regress(v)[2],\n            \"t_test\": compare_ttest_regress(v)[3],\n            \"regress\": compare_ttest_regress(v)[4],\n        }\n    )\n\npd.set_option(\"display.float_format\", \"{:.3f}\".format)\n\ndf_result = pd.DataFrame(results)\ndf_result\n\n\n\n\n\n\n\n\n\nvariable\ncontrol mean\ntreatment mean\ndifference\nt_test\nregress\n\n\n\n\n0\nhpa\n59.597\n58.960\n0.637\n0.970\n0.970\n\n\n1\nmrm2\n13.012\n12.998\n0.014\n0.120\n0.120\n\n\n2\nfreq\n8.035\n8.047\n-0.012\n-0.110\n-0.110\n\n\n3\nyears\n6.078\n6.136\n-0.058\n-1.090\n-1.090\n\n\n4\nyear5\n0.506\n0.514\n-0.007\n-1.560\n-1.560\n\n\n5\ndormant\n0.524\n0.523\n0.001\n0.170\n0.170\n\n\n\n\n\n\n\nAcross all selected variables, the differences between treatment and control groups are small. The t-test statistics and the regression coefficients for all variables are also small, indicating that the differences between the two groups are not statistically significant.\nThere is no evidence that there are selecting bias existing, which supports the validity of the experimental design and suggests that the selection was successfully random."
  },
  {
    "objectID": "Projects/project1/index.html#experimental-results",
    "href": "Projects/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\nCode\nimport matplotlib.pyplot as plt\n\n\ndf_treat = df.query(\"treatment==1\")\ndf_control = df.query(\"control==1\")\ntreat_prop = df_treat[\"gave\"].value_counts(normalize=True)\ncontrol_prop = df_control[\"gave\"].value_counts(normalize=True)\n\nfig, ax = plt.subplots()\nbars = ax.bar(\n    [\"treatment\", \"control\"],\n    [treat_prop[1] * 100, control_prop[1] * 100],\n    color=[\"skyblue\", \"orange\"],\n)\n\nfor bar in bars:\n    height = bar.get_height()\n    ax.annotate(\n        f\"{height:.1f}%\",\n        xy=(bar.get_x() + bar.get_width() / 2, height),\n        xytext=(0, 2),\n        textcoords=\"offset points\",\n        ha=\"center\",\n        va=\"bottom\",\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\ntreat = df[df[\"treatment\"] == 1][\"gave\"]\ncontrol = df[df[\"treatment\"] == 0][\"gave\"]\n# t-test\nmean_diff = treat.mean() - control.mean()\nvar_treat = treat.var(ddof=1)\nvar_control = control.var(ddof=1)\nse = np.sqrt(var_treat / len(treat) + var_control / len(control))\nt_stat = mean_diff / se\nttest_result = round(t_stat, 2)\nprint(f\"T-test result: {ttest_result}\")\n\n\nT-test result: 3.21\n\n\nSince the t-test result is so large, we can conclude that the treatment group is significantly more willing to donate than the control group.\n\n\nCode\n# regression\nX = df[\"treatment\"].values.reshape(-1, 1)\ny = df[\"gave\"].values\nreg = linear_model.LinearRegression()\nreg.fit(X, y)\nreg_result = round(reg.coef_[0] / se, 2)\nprint(f\"Regression result: {reg_result}\")\n\n\nRegression result: 3.21\n\n\nAccording to the linear regression model, the coefficient is so big that we can conclude that the treatment group is more likely to donate than the control group.\n\n\nCode\nimport statsmodels.api as sm\nfrom statsmodels.discrete.discrete_model import Probit\n\ndf_clean = df[[\"gave\", \"treatment\"]].dropna()\n\nX = sm.add_constant(df_clean[\"treatment\"])\ny = df_clean[\"gave\"]\nprobit_model = sm.Probit(y, X).fit()\n\name = probit_model.get_margeff(at=\"overall\").summary_frame()\name.round(3)\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\n\n\n\ndy/dx\nStd. Err.\nz\nPr(&gt;|z|)\nConf. Int. Low\nCont. Int. Hi.\n\n\n\n\ntreatment\n0.004\n0.001\n3.104\n0.002\n0.002\n0.007\n\n\n\n\n\n\n\nThe probit regression results also align with the t-test and regression results. It indicates that the treatment group has a statistically significant effect on the willingness of donating. The marginal effect of treatment is about 0.0043, meaning the treatment increases the probability of donation by approximately 0.43%.\nThe z-score is 3.104 with a p-value of 0.002, providing strong evidence that the difference is not due to random chance, further supports the conclusion that matched donations significantly increase donor participation.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate. There are three matched donation ratios: 1-1, 2-1, 3-1\n\n\nCode\nfrom scipy import stats\n\nratio1 = df.query(\"ratio==1\")[\"gave\"]\nratio2 = df.query(\"ratio2==1\")[\"gave\"]\nratio3 = df.query(\"ratio3==1\")[\"gave\"]\n\n\n# t-test\ndef t_test(df1, df2):\n    mean_diff = df1.mean() - df2.mean()\n    var1 = df1.var(ddof=1)\n    var2 = df2.var(ddof=1)\n    se = np.sqrt(var1 / len(df1) + var2 / len(df2))\n    t_stat = mean_diff / se\n    ttest_result = round(t_stat, 2)\n    numerator = (var1 / len(df1) + var2 / len(df2)) ** 2\n    denominator = ((var1 / len(df1)) ** 2) / (len(df1) - 1) + (\n        (var2 / len(df2)) ** 2\n    ) / (len(df2) - 1)\n    degree_of_freedom = numerator / denominator\n\n    p_value = stats.t.sf(np.abs(t_stat), degree_of_freedom) * 2\n\n    return ttest_result, round(p_value, 2)\n\n\nprint(\n    f\"1:1 vs 2:1: t-test {t_test(ratio2, ratio1)[0]}, p-value {t_test(ratio2, ratio1)[1]}\"\n)\nprint(\n    f\"2:1 vs 3:1: t-test {t_test(ratio3, ratio2)[0]}, p-value {t_test(ratio3, ratio2)[1]}\"\n)\nprint(\n    f\"1:1 vs 3:1: t-test {t_test(ratio3, ratio1)[0]}, p-value {t_test(ratio3, ratio1)[1]}\"\n)\n\n\n1:1 vs 2:1: t-test 0.97, p-value 0.33\n2:1 vs 3:1: t-test 0.05, p-value 0.96\n1:1 vs 3:1: t-test 1.02, p-value 0.31\n\n\nThe two-sample t-tests comparing match ratios (1:1 vs 2:1, 2:1 vs 3:1, and 1:1 vs 3:1) yield no statistically significant differences, with p-values all above 0.05. This suggests that, based on pairwise comparisons, larger match ratios do not significantly increase the likelihood of donation.\n\n\nCode\nimport statsmodels.formula.api as smf\n\nX = df[[\"ratio2\", \"ratio3\"]]\nX = sm.add_constant(X)\ny = df[\"gave\"]\npd.set_option(\"display.float_format\", \"{:.3f}\".format)\nmodel = smf.ols(\"gave~ratio2+ratio3-1\", data=df).fit()\nmodel.summary2().tables[1].round(3)\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nratio2\n0.023\n0.001\n16.714\n0.000\n0.020\n0.025\n\n\nratio3\n0.023\n0.001\n16.784\n0.000\n0.020\n0.025\n\n\n\n\n\n\n\nThe OLS regression results show that both the 2:1 and 3:1 match conditions have statistically significant positive coefficients (0.023), indicating that individuals in these groups are more likely to donate compared to the 1:1 baseline. However, since the coefficients for 2:1 and 3:1 are nearly identical, this suggests that raising the match ratio from 2:1 to 3:1 does not lead to any additional gain.\nT-tests compare two groups directly and are more conservative when sample sizes differ, whereas the regression model uses all data and estimates relative effects simultaneously, leading to greater statistical power. Importantly, both methods agree on the core insight: offering a match increases donations, but increasing the size of the match (beyond 1:1) does not matter.\nI further looked into the mean probabilities of making donations among ratio 1:1, 2:1, and 3:1.\n\n\nCode\nratio1_mean = df.query(\"ratio==1\")[\"gave\"].mean()\nratio2_mean = df.query(\"ratio2==1\")[\"gave\"].mean()\nratio3_mean = df.query(\"ratio3==1\")[\"gave\"].mean()\n\ndiff11_21 = ratio2_mean - ratio1_mean\ndiff21_31 = ratio3_mean - ratio2_mean\n\nprint(f\"Difference in mean between 2:1 and 1:1 = {diff11_21:.4f}\")\nprint(f\"Difference in mean between 3:1 and 2:1 = {diff21_31:.4f}\")\n\n\nDifference in mean between 2:1 and 1:1 = 0.0019\nDifference in mean between 3:1 and 2:1 = 0.0001\n\n\nWhen calculating the response rate directly from the data, the difference between the 2:1 and 1:1 match ratios is about 0.19%, while the difference between 3:1 and 2:1 is only 0.01%. These results are consistent with the coefficients from the OLS regression, where ratio2 and ratio3 have coefficients of 0.0036 and 0.0037 respectively.\nIn conclusion, increasing the match ratio from 1:1 to 2:1 appears to have a small positive effect on donation likelihood, but increasing the match ratio further to 3:1 shows no additional gain.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nT-test\n\n\nCode\ntreat_amount = df.query(\"treatment == 1\")[\"amount\"]\ncontrol_amount = df.query(\"control == 1\")[\"amount\"]\nprint(\n    f\"t_stats: {t_test(treat_amount, control_amount)[0]:.2f}, p-value: {t_test(treat_amount, control_amount)[1]:.2f}\"\n)\n\n\nt_stats: 1.92, p-value: 0.06\n\n\nRegression\n\n\nCode\nX = sm.add_constant(df[\"treatment\"])\ny = df[\"amount\"]\nmodel = smf.ols(\"amount ~ treatment\", data=df).fit()\ncoef_table = model.summary2().tables[1]\nprint(coef_table)\n\n\n           Coef.  Std.Err.      t  P&gt;|t|  [0.025  0.975]\nIntercept  0.813     0.067 12.063  0.000   0.681   0.945\ntreatment  0.154     0.083  1.861  0.063  -0.008   0.315\n\n\nI conduct a t-test and a bivariate regression of the donation amount on treatment assignment. The results show that treatment group donors gave approximately $0.15 more on average than the control group, but the difference is small that the p-value wasn’t statistically significant, meaning that the impact is limited.\nI then looked into the sample that made a donation and did the regression analysis of donation amount on treatment assignment.\n\n\nCode\ndf_gave = df.query(\"gave==1\")[[\"treatment\", \"amount\"]]\n\nmodel = smf.ols(\"amount ~ treatment\", data=df_gave).fit()\ncoef_table = model.summary2().tables[1]\nprint(coef_table)\n\n\n           Coef.  Std.Err.      t  P&gt;|t|  [0.025  0.975]\nIntercept 45.540     2.423 18.792  0.000  40.785  50.296\ntreatment -1.668     2.872 -0.581  0.561  -7.305   3.968\n\n\nThe coefficient on treatment is −1.67 with a p-value of 0.561, indicating no statistically significant difference in donation amounts between the treatment and control groups, suggesting that the treatment group only, on average, gave slightly less than the control group.\nHowever, the coefficient does not allow a causal interpretation because adding conditions on treatment behavior (donating) introduces potential selection bias.\n\n\nCode\nfig, ax = plt.subplots()\nbars = ax.bar(\n    [\"treatment\", \"control\"],\n    [\n        df_gave.query(\"treatment==1\")[\"amount\"].mean(),\n        df_gave.query(\"treatment==0\")[\"amount\"].mean(),\n    ],\n    color=[\"skyblue\", \"orange\"],\n)\n\nfor bar in bars:\n    height = bar.get_height()\n    ax.annotate(\n        f\"${height:.2f}\",\n        xy=(bar.get_x() + bar.get_width() / 2, height),\n        xytext=(0, 2),\n        textcoords=\"offset points\",\n        ha=\"center\",\n        va=\"bottom\",\n    )\n\nax.set_ylabel(\"Average Donation Amount ($)\")\nax.set_title(\"Average Donation Amount\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart compares the average donation amounts between treatment and control groups, conditional on having donated. The control group gave slightly more on average ($45.54) than the treatment group ($43.87)."
  },
  {
    "objectID": "Projects/project1/index.html#simulation-experiment",
    "href": "Projects/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\nCode\np_control = 0.018\np_treat = 0.022\nn_sim = 10_000\n\nnp.random.seed(100)\ncontrol_draw = np.random.binomial(1, p_control, n_sim)\ntreat_draw = np.random.binomial(1, p_treat, n_sim)\n\ndiff = treat_draw - control_draw\ncumulative_avg = np.cumsum(diff) / np.arange(1, n_sim + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label=\"Cumulative Avg of Difference: Treatment - Control\")\nplt.axhline(p_treat - p_control, color=\"red\", linestyle=\"--\", label=\"True Mean Diff\")\nplt.title(\"Simulation of Cumulative Average Difference\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe figure shows the cumulative average difference between treatment and control groups from a simulation. The average converges toward the true difference (0.004) as the number of simulations gets larger.\nThis illustrates the Law of Large Numbers: as the number of simulations increases, the observed mean gets closer to the expected (true) mean.\n\n\nCentral Limit Theorem\n\n\nCode\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    np.random.seed(100)\n    for _ in range(n_sim):\n        c = np.random.binomial(1, p_control, n).mean()\n        t = np.random.binomial(1, p_treat, n).mean()\n        diffs.append(t - c)\n\n    axs[i].hist(diffs, bins=30, color=\"lightblue\", edgecolor=\"black\")\n    axs[i].axvline(0, color=\"red\", linestyle=\"--\", label=\"Zero Line\")\n    axs[i].axvline(np.mean(diffs), color=\"green\", linestyle=\"-\", label=\"Mean Diff\")\n    axs[i].set_title(f\"Sample size = {n}\")\n    axs[i].set_xlabel(\"Treatment - Control Mean Diff\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\nplt.suptitle(\"Central Limit Theorem: Distribution of Mean Differences\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs the sample size gets larger (50, 200, 500, 1,000), the distribution of average differences will be narrower, more symmetric, and closer to normal distribution.\nFor small samples (e.g., n = 50), the distribution is noisy and is closer to 0. As n becomes larger (e.g., n = 1,000), the distribution converges to normal and centers around the true effect (0.04)."
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nKuan-Ling (Rebecca) Tseng\n\n\nMay 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nYour Name\n\n\nMay 3, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Projects/project2/index.html",
    "href": "Projects/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"../../data/blueprinty.csv\")\n\nprint(f\"The dataset contains: {data.shape[0]:,} rows\")\n\ndata.head(3)\n\n\nThe dataset contains: 1,500 rows\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n\n\n\n\n\nThe dataset involves 1,500 rows(engineering firms), with 4 columns.\nThe histogram compares the distribution of the number of patents of customers and non-customers\nWe observe that Blueprinty customers tend to hold more patents on average compared to non-customers.\n\n\nCode\nplt.hist(\n    [data.query(\"iscustomer==1\")[\"patents\"], data.query(\"iscustomer==0\")[\"patents\"]]\n)\nplt.title(\"Distribution of Number of Patents\")\nplt.xlabel(\"Number of patents\")\nplt.ylabel(\"Count\")\nlabels = [\"Customer\", \"Non-customer\"]\nplt.legend(labels)\n\nmean_customer_patent = data.query(\"iscustomer==1\")[\"patents\"].mean()\nmean_noncustomer_patent = data.query(\"iscustomer==0\")[\"patents\"].mean()\n\nprint(f\"Means of number of patents of customers: {mean_customer_patent:.2f}\")\nprint(f\"Means of number of patents of non-customers: {mean_noncustomer_patent:.2f}\")\n\n\nMeans of number of patents of customers: 4.13\nMeans of number of patents of non-customers: 3.47\n\n\n\n\n\n\n\n\n\n\nThe mean number of patents among customers is 4.13, while for non-customers it is 3.47.\nThe distribution for non-customers is more concentrated around 2–3 patents, whereas customers show a heavier tail, indicating that a greater proportion of them hold higher numbers of patents.\nThere are also more customers with patent counts above 6 compared to non-customers.\n\nThese findings suggest that Blueprinty customers are more likely to be high-output or more innovation-active entities. This could imply that the company is either targeting or attracting organizations with greater innovative capacity.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nThe bar chart shows the geographic distribution of Blueprinty customers and non-customers across five U.S. regions\n\n\nCode\ncustomer_counts = data.query(\"iscustomer == 1\")[\"region\"].value_counts().sort_index()\nnoncustomer_counts = data.query(\"iscustomer == 0\")[\"region\"].value_counts().sort_index()\n\nregions = customer_counts.index\nx = np.arange(len(regions))\nwidth = 0.35\n\nplt.bar(x - width / 2, customer_counts, width=width, label=\"Customer\")\nplt.bar(x + width / 2, noncustomer_counts, width=width, label=\"Non-customer\")\n\nplt.xticks(x, regions)\nplt.xlabel(\"Regions\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Regions by Customer Status\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNortheast has the largest customer base: It has the highest count of customers and more customers than non-customers in this region, suggesting that Blueprinty has strong market penetration there.\nOverall, non-customers are a lot more than customers in every other region: The Northwest, South, Southwest, and Midwest all have significantly more non-customers than customers, showing high market potential.\n\nThese results imply that Blueprinty’s customer acquisition is highly regionally skewed, particularly concentrated in the Northeast. This could be due to marketing focus, proximity to headquarters, existing network effects, or simply regional product-market fit.\nThe histogram illustrates the age distribution of Blueprinty customers and non-customers\nThe mean age for customers is 26.90, slightly higher than the 26.10 average for non-customers.\n\n\nCode\ncustomer_age = data.query(\"iscustomer==1\")[\"age\"]\nnoncustomer_age = data.query(\"iscustomer==0\")[\"age\"]\n\nbins = np.arange(10, 51, 2)\n\nplt.hist(customer_age, bins=bins, alpha=0.9, label=\"Customer\", color=\"salmon\")\nplt.hist(noncustomer_age, bins=bins, alpha=0.7, label=\"Non-customer\", color=\"skyblue\")\n\nplt.title(\"Distribution of Age\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.legend()\n\nmean_customer_age = customer_age.mean()\nmean_noncustomer_age = noncustomer_age.mean()\n\nprint(f\"Mean age of customers: {mean_customer_age:.2f}\")\nprint(f\"Mean age of non-customers: {mean_noncustomer_age:.2f}\")\n\nplt.tight_layout()\nplt.show()\n\n\nMean age of customers: 26.90\nMean age of non-customers: 26.10\n\n\n\n\n\n\n\n\n\n\nThe overall distributions are quite similar, with both groups peaking around ages 20 to 30.\nHowever, non-customers are more concentrated in the 18–30 age range, especially at the peak (age ~25).\nCustomers have a slightly broader spread, with a relatively higher proportion in the 30–45 age range.\n\nThis suggests that while the average age is similar, Blueprinty customers may skew marginally older, potentially indicating different needs, purchasing power, or product fit among slightly older users.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nMathematical function for the likelihood (or log-likelihood)\n\\(L(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\\)\nCode for the likelihood (or log-likelihood) function for the Poisson model\n\nfrom scipy.special import gammaln\n\n\ndef poisson_loglikelihood(l, Y):\n    log_likelihood = np.sum(-l + Y * np.log(l) - gammaln(Y + 1))\n    return log_likelihood\n\n\n\nCode\nY = data[\"patents\"]\n\nlambdas = np.linspace(0.1, 10, 100)\nlog_likelihoods = [poisson_loglikelihood(l, Y) for l in lambdas]\n\nplt.plot(lambdas, log_likelihoods)\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood Curve (input with # of patents)\")\nplt.axvline(np.mean(Y), color=\"salmon\", linestyle=\"--\", label=\"Mean(Y)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nWhen taking the first derivative of the likelihood or log-likelihood function, set it equal to zero, and solve for lambda.\n\\(\\frac{d}{d\\lambda} \\log L = -n + \\frac{\\sum y_i}{\\lambda} = 0 \\Rightarrow \\hat{\\lambda} = \\frac{1}{n} \\sum y_i = \\bar{y}\\)\nThen, we can further calculate the MLE by optimizing the likelihood function.\n\n\nCode\nfrom scipy.optimize import minimize_scalar\n\n\ndef neg_loglikelihood(lmbda):\n    return -poisson_loglikelihood(lmbda, Y)\n\n\nresult = minimize_scalar(neg_loglikelihood, bounds=(0.1, 10), method=\"bounded\")\nprint(f\"MLE (lambda): {result.x: .2f}\")\n\n\nMLE (lambda):  3.68\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nUpdated Function\n\\(\\log L(\\beta) = \\sum_{i=1}^n \\left[ y_i \\log \\lambda_i - \\lambda_i - \\log(y_i!) \\right]= \\sum_{i=1}^n \\left[ y_i X_i^\\top \\beta - \\exp(X_i^\\top \\beta) - \\log(y_i!) \\right]\\)\nUpdated code for the function\n\ndef poisson_loglikelihood2(beta, Y, X):\n    lambda_i = np.exp(X @ beta)\n    return np.sum(Y * np.log(lambda_i) - lambda_i - gammaln(Y + 1))\n\n\ndef neg_loglikelihood2(beta, Y, X):\n    return -poisson_loglikelihood2(beta, Y, X)\n\nThen, I can get the MLE vector and the Hessian of the Poisson model with covariates. The first column of X should is 1s to enable a constant term in the model, and the subsequent columns are age, age squared, binary variables for all but one of the regions, and the binary customer variable.\n\nfrom scipy.optimize import minimize\n\ndata[\"age_z\"] = (data[\"age\"] - data[\"age\"].mean()) / data[\"age\"].std()\ndata[\"age_sq_z\"] = data[\"age_z\"] ** 2\nregion_dummies = pd.get_dummies(data[\"region\"], prefix=\"region\", drop_first=True)\nX_data = pd.concat(\n    [\n        pd.Series(1, index=data.index, name=\"const\"),\n        data[[\"age\", \"age_sq_z\", \"iscustomer\"]],\n        region_dummies,\n    ],\n    axis=1,\n)\n\nX = X_data.values.astype(float)\nY = data[\"patents\"].values\n\ninit_beta = np.zeros(X.shape[1], dtype=float)\nresult = minimize(neg_loglikelihood2, x0=init_beta, args=(Y, X), method=\"BFGS\")\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nThe coefficients and standard errors of the beta parameter estimates using Hessian\n\n\nCode\nsummary = pd.DataFrame(\n    {\n        \"Variable\": X_data.columns,\n        \"Coefficient\": np.round(beta_hat, 3),\n        \"Std. Error\": np.round(std_errors, 3),\n    }\n)\nsummary\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\n0\nconst\n1.555\n0.085\n\n\n1\nage\n-0.008\n0.003\n\n\n2\nage_sq_z\n-0.156\n0.014\n\n\n3\niscustomer\n0.208\n0.031\n\n\n4\nregion_Northeast\n0.029\n0.028\n\n\n5\nregion_Northwest\n-0.018\n0.036\n\n\n6\nregion_South\n0.057\n0.042\n\n\n7\nregion_Southwest\n0.051\n0.034\n\n\n\n\n\n\n\nTo check the results, I used sm.GLM() and received similar numbers\n\n\nCode\nimport statsmodels.api as sm\nimport pandas as pd\n\nX_data_glm = sm.add_constant(X_data.drop(columns=\"const\").astype(float))\nY = data[\"patents\"].astype(float).values\n\n\nglm_model = sm.GLM(Y, X_data_glm, family=sm.families.Poisson())\nglm_result = glm_model.fit()\n\n\nsummary_table = pd.DataFrame(\n    {\n        \"coef\": glm_result.params,\n        \"std err\": glm_result.bse,\n        \"z\": glm_result.tvalues,\n        \"P&gt;|z|\": glm_result.pvalues,\n        \"[0.025\": glm_result.conf_int()[0],\n        \"0.975]\": glm_result.conf_int()[1],\n    }\n)\n\nsummary_table.round(3)\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nconst\n1.555\n0.066\n23.437\n0.000\n1.425\n1.685\n\n\nage\n-0.008\n0.002\n-3.843\n0.000\n-0.012\n-0.004\n\n\nage_sq_z\n-0.156\n0.014\n-11.513\n0.000\n-0.182\n-0.129\n\n\niscustomer\n0.208\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nregion_Northeast\n0.029\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nregion_Northwest\n-0.018\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nregion_South\n0.057\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nregion_Southwest\n0.051\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\n\n\nThe Poisson regression results indicate that age and customer statusare statistically significant predictors of the number of patents among engineering firms, based on the p-values (P &gt;| z|). Moreover, the relationship between age and patent count follows a concave shape, suggesting that innovation returns initially increase with firm age but begin to decline after reaching a peak.\nAdditionally, Blueprinty customers are estimated to produce approximately 23% (\\(e^{0.208} - 1 \\approx 23.1\\%\\)) more patents than non-customers, when all other features remain the same. Regional differences do not appear to have a statistically significant impact on patent production.\nTo conclude about the effect of Blueprinty’s software on patent success, I calculated the counterfactual prediction.\n\n\nCode\niscust_idx = list(X_data.columns).index(\"iscustomer\")\n\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[:, iscust_idx] = 0\nX_1[:, iscust_idx] = 1\n\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\ndelta = y_pred_1 - y_pred_0\navg_diff = delta.mean()\n\nprint(\n    f\"Average increase in expected number of patents due to being a Blueprinty customer: {avg_diff:.2f}\"\n)\n\n\nAverage increase in expected number of patents due to being a Blueprinty customer: 0.79\n\n\nBased on the Poisson regression model, being a Blueprinty customer increases the expected number of patents by approximately 0.79 patents per firm, making all other features constant."
  },
  {
    "objectID": "Projects/project2/index.html#blueprinty-case-study",
    "href": "Projects/project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"../../data/blueprinty.csv\")\n\nprint(f\"The dataset contains: {data.shape[0]:,} rows\")\n\ndata.head(3)\n\n\nThe dataset contains: 1,500 rows\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n\n\n\n\n\nThe dataset involves 1,500 rows(engineering firms), with 4 columns.\nThe histogram compares the distribution of the number of patents of customers and non-customers\nWe observe that Blueprinty customers tend to hold more patents on average compared to non-customers.\n\n\nCode\nplt.hist(\n    [data.query(\"iscustomer==1\")[\"patents\"], data.query(\"iscustomer==0\")[\"patents\"]]\n)\nplt.title(\"Distribution of Number of Patents\")\nplt.xlabel(\"Number of patents\")\nplt.ylabel(\"Count\")\nlabels = [\"Customer\", \"Non-customer\"]\nplt.legend(labels)\n\nmean_customer_patent = data.query(\"iscustomer==1\")[\"patents\"].mean()\nmean_noncustomer_patent = data.query(\"iscustomer==0\")[\"patents\"].mean()\n\nprint(f\"Means of number of patents of customers: {mean_customer_patent:.2f}\")\nprint(f\"Means of number of patents of non-customers: {mean_noncustomer_patent:.2f}\")\n\n\nMeans of number of patents of customers: 4.13\nMeans of number of patents of non-customers: 3.47\n\n\n\n\n\n\n\n\n\n\nThe mean number of patents among customers is 4.13, while for non-customers it is 3.47.\nThe distribution for non-customers is more concentrated around 2–3 patents, whereas customers show a heavier tail, indicating that a greater proportion of them hold higher numbers of patents.\nThere are also more customers with patent counts above 6 compared to non-customers.\n\nThese findings suggest that Blueprinty customers are more likely to be high-output or more innovation-active entities. This could imply that the company is either targeting or attracting organizations with greater innovative capacity.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nThe bar chart shows the geographic distribution of Blueprinty customers and non-customers across five U.S. regions\n\n\nCode\ncustomer_counts = data.query(\"iscustomer == 1\")[\"region\"].value_counts().sort_index()\nnoncustomer_counts = data.query(\"iscustomer == 0\")[\"region\"].value_counts().sort_index()\n\nregions = customer_counts.index\nx = np.arange(len(regions))\nwidth = 0.35\n\nplt.bar(x - width / 2, customer_counts, width=width, label=\"Customer\")\nplt.bar(x + width / 2, noncustomer_counts, width=width, label=\"Non-customer\")\n\nplt.xticks(x, regions)\nplt.xlabel(\"Regions\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Regions by Customer Status\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nNortheast has the largest customer base: It has the highest count of customers and more customers than non-customers in this region, suggesting that Blueprinty has strong market penetration there.\nOverall, non-customers are a lot more than customers in every other region: The Northwest, South, Southwest, and Midwest all have significantly more non-customers than customers, showing high market potential.\n\nThese results imply that Blueprinty’s customer acquisition is highly regionally skewed, particularly concentrated in the Northeast. This could be due to marketing focus, proximity to headquarters, existing network effects, or simply regional product-market fit.\nThe histogram illustrates the age distribution of Blueprinty customers and non-customers\nThe mean age for customers is 26.90, slightly higher than the 26.10 average for non-customers.\n\n\nCode\ncustomer_age = data.query(\"iscustomer==1\")[\"age\"]\nnoncustomer_age = data.query(\"iscustomer==0\")[\"age\"]\n\nbins = np.arange(10, 51, 2)\n\nplt.hist(customer_age, bins=bins, alpha=0.9, label=\"Customer\", color=\"salmon\")\nplt.hist(noncustomer_age, bins=bins, alpha=0.7, label=\"Non-customer\", color=\"skyblue\")\n\nplt.title(\"Distribution of Age\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.legend()\n\nmean_customer_age = customer_age.mean()\nmean_noncustomer_age = noncustomer_age.mean()\n\nprint(f\"Mean age of customers: {mean_customer_age:.2f}\")\nprint(f\"Mean age of non-customers: {mean_noncustomer_age:.2f}\")\n\nplt.tight_layout()\nplt.show()\n\n\nMean age of customers: 26.90\nMean age of non-customers: 26.10\n\n\n\n\n\n\n\n\n\n\nThe overall distributions are quite similar, with both groups peaking around ages 20 to 30.\nHowever, non-customers are more concentrated in the 18–30 age range, especially at the peak (age ~25).\nCustomers have a slightly broader spread, with a relatively higher proportion in the 30–45 age range.\n\nThis suggests that while the average age is similar, Blueprinty customers may skew marginally older, potentially indicating different needs, purchasing power, or product fit among slightly older users.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nMathematical function for the likelihood (or log-likelihood)\n\\(L(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\\)\nCode for the likelihood (or log-likelihood) function for the Poisson model\n\nfrom scipy.special import gammaln\n\n\ndef poisson_loglikelihood(l, Y):\n    log_likelihood = np.sum(-l + Y * np.log(l) - gammaln(Y + 1))\n    return log_likelihood\n\n\n\nCode\nY = data[\"patents\"]\n\nlambdas = np.linspace(0.1, 10, 100)\nlog_likelihoods = [poisson_loglikelihood(l, Y) for l in lambdas]\n\nplt.plot(lambdas, log_likelihoods)\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood Curve (input with # of patents)\")\nplt.axvline(np.mean(Y), color=\"salmon\", linestyle=\"--\", label=\"Mean(Y)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nWhen taking the first derivative of the likelihood or log-likelihood function, set it equal to zero, and solve for lambda.\n\\(\\frac{d}{d\\lambda} \\log L = -n + \\frac{\\sum y_i}{\\lambda} = 0 \\Rightarrow \\hat{\\lambda} = \\frac{1}{n} \\sum y_i = \\bar{y}\\)\nThen, we can further calculate the MLE by optimizing the likelihood function.\n\n\nCode\nfrom scipy.optimize import minimize_scalar\n\n\ndef neg_loglikelihood(lmbda):\n    return -poisson_loglikelihood(lmbda, Y)\n\n\nresult = minimize_scalar(neg_loglikelihood, bounds=(0.1, 10), method=\"bounded\")\nprint(f\"MLE (lambda): {result.x: .2f}\")\n\n\nMLE (lambda):  3.68\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nUpdated Function\n\\(\\log L(\\beta) = \\sum_{i=1}^n \\left[ y_i \\log \\lambda_i - \\lambda_i - \\log(y_i!) \\right]= \\sum_{i=1}^n \\left[ y_i X_i^\\top \\beta - \\exp(X_i^\\top \\beta) - \\log(y_i!) \\right]\\)\nUpdated code for the function\n\ndef poisson_loglikelihood2(beta, Y, X):\n    lambda_i = np.exp(X @ beta)\n    return np.sum(Y * np.log(lambda_i) - lambda_i - gammaln(Y + 1))\n\n\ndef neg_loglikelihood2(beta, Y, X):\n    return -poisson_loglikelihood2(beta, Y, X)\n\nThen, I can get the MLE vector and the Hessian of the Poisson model with covariates. The first column of X should is 1s to enable a constant term in the model, and the subsequent columns are age, age squared, binary variables for all but one of the regions, and the binary customer variable.\n\nfrom scipy.optimize import minimize\n\ndata[\"age_z\"] = (data[\"age\"] - data[\"age\"].mean()) / data[\"age\"].std()\ndata[\"age_sq_z\"] = data[\"age_z\"] ** 2\nregion_dummies = pd.get_dummies(data[\"region\"], prefix=\"region\", drop_first=True)\nX_data = pd.concat(\n    [\n        pd.Series(1, index=data.index, name=\"const\"),\n        data[[\"age\", \"age_sq_z\", \"iscustomer\"]],\n        region_dummies,\n    ],\n    axis=1,\n)\n\nX = X_data.values.astype(float)\nY = data[\"patents\"].values\n\ninit_beta = np.zeros(X.shape[1], dtype=float)\nresult = minimize(neg_loglikelihood2, x0=init_beta, args=(Y, X), method=\"BFGS\")\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\nThe coefficients and standard errors of the beta parameter estimates using Hessian\n\n\nCode\nsummary = pd.DataFrame(\n    {\n        \"Variable\": X_data.columns,\n        \"Coefficient\": np.round(beta_hat, 3),\n        \"Std. Error\": np.round(std_errors, 3),\n    }\n)\nsummary\n\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\n0\nconst\n1.555\n0.085\n\n\n1\nage\n-0.008\n0.003\n\n\n2\nage_sq_z\n-0.156\n0.014\n\n\n3\niscustomer\n0.208\n0.031\n\n\n4\nregion_Northeast\n0.029\n0.028\n\n\n5\nregion_Northwest\n-0.018\n0.036\n\n\n6\nregion_South\n0.057\n0.042\n\n\n7\nregion_Southwest\n0.051\n0.034\n\n\n\n\n\n\n\nTo check the results, I used sm.GLM() and received similar numbers\n\n\nCode\nimport statsmodels.api as sm\nimport pandas as pd\n\nX_data_glm = sm.add_constant(X_data.drop(columns=\"const\").astype(float))\nY = data[\"patents\"].astype(float).values\n\n\nglm_model = sm.GLM(Y, X_data_glm, family=sm.families.Poisson())\nglm_result = glm_model.fit()\n\n\nsummary_table = pd.DataFrame(\n    {\n        \"coef\": glm_result.params,\n        \"std err\": glm_result.bse,\n        \"z\": glm_result.tvalues,\n        \"P&gt;|z|\": glm_result.pvalues,\n        \"[0.025\": glm_result.conf_int()[0],\n        \"0.975]\": glm_result.conf_int()[1],\n    }\n)\n\nsummary_table.round(3)\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nconst\n1.555\n0.066\n23.437\n0.000\n1.425\n1.685\n\n\nage\n-0.008\n0.002\n-3.843\n0.000\n-0.012\n-0.004\n\n\nage_sq_z\n-0.156\n0.014\n-11.513\n0.000\n-0.182\n-0.129\n\n\niscustomer\n0.208\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nregion_Northeast\n0.029\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nregion_Northwest\n-0.018\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nregion_South\n0.057\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nregion_Southwest\n0.051\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\n\n\nThe Poisson regression results indicate that age and customer statusare statistically significant predictors of the number of patents among engineering firms, based on the p-values (P &gt;| z|). Moreover, the relationship between age and patent count follows a concave shape, suggesting that innovation returns initially increase with firm age but begin to decline after reaching a peak.\nAdditionally, Blueprinty customers are estimated to produce approximately 23% (\\(e^{0.208} - 1 \\approx 23.1\\%\\)) more patents than non-customers, when all other features remain the same. Regional differences do not appear to have a statistically significant impact on patent production.\nTo conclude about the effect of Blueprinty’s software on patent success, I calculated the counterfactual prediction.\n\n\nCode\niscust_idx = list(X_data.columns).index(\"iscustomer\")\n\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[:, iscust_idx] = 0\nX_1[:, iscust_idx] = 1\n\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\ndelta = y_pred_1 - y_pred_0\navg_diff = delta.mean()\n\nprint(\n    f\"Average increase in expected number of patents due to being a Blueprinty customer: {avg_diff:.2f}\"\n)\n\n\nAverage increase in expected number of patents due to being a Blueprinty customer: 0.79\n\n\nBased on the Poisson regression model, being a Blueprinty customer increases the expected number of patents by approximately 0.79 patents per firm, making all other features constant."
  },
  {
    "objectID": "Projects/project2/index.html#airbnb-case-study",
    "href": "Projects/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nThe Airbnb dataset involves 40,628 rows, with 13 rows.\n\n\nCode\ndata = pd.read_csv(\"../../data/airbnb.csv\", index_col=0)\n\nprint(f\"The dataset contains: {data.shape[0]:,} rows\")\n\ndata.head(3)\n\n\nThe dataset contains: 40,628 rows\n\n\n\n\n\n\n\n\n\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n\n\n\n\n\nExploratory Data Analysis\n\n\nCode\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18, 14))\naxes = axes.flatten()\n\ndays = data[\"days\"].dropna()\nbathrooms = data[\"bathrooms\"].dropna()\nbedrooms = data[\"bedrooms\"].dropna()\nprice = data[\"price\"].dropna()\ncleanliness = data[\"review_scores_cleanliness\"].dropna()\nlocation = data[\"review_scores_location\"].dropna()\nvalue = data[\"review_scores_value\"].dropna()\nreviews = data[\"number_of_reviews\"].dropna()\n\n# days- Number of Days Listed\naxes[0].boxplot(days)\naxes[0].set_title(\"Number of Days Listed\")\naxes[0].set_ylabel(\"Days\")\n\n# bathrooms\naxes[1].hist(bathrooms, bins=15)\naxes[1].set_title(\"Number of Bathrooms\")\naxes[1].set_xlabel(\"Bathrooms\")\naxes[1].set_ylabel(\"Count\")\n\n# bedrooms\naxes[2].hist(bedrooms, bins=15)\naxes[2].set_title(\"Number of Bedrooms\")\naxes[2].set_xlabel(\"Bedrooms\")\naxes[2].set_ylabel(\"Count\")\n\n# price\naxes[3].boxplot(price)\naxes[3].set_title(\"Price Distribution\")\naxes[3].set_ylabel(\"Price\")\n\n# review_scores_cleanliness\naxes[4].hist(cleanliness, bins=10)\naxes[4].set_title(\"Review Scores: Cleanliness\")\naxes[4].set_xlabel(\"Score\")\naxes[4].set_ylabel(\"Count\")\n\n# review_scores_location\naxes[5].hist(location, bins=10)\naxes[5].set_title(\"Review Scores: Location\")\naxes[5].set_xlabel(\"Score\")\naxes[5].set_ylabel(\"Count\")\n\n# review_scores_value\naxes[6].hist(location, bins=10)\naxes[6].set_title(\"Review Scores: Value\")\naxes[6].set_xlabel(\"Score\")\naxes[6].set_ylabel(\"Count\")\n\n# Instant Bookable vs Reviews\nbookable = data.query(\"instant_bookable == 't'\")[\"number_of_reviews\"].dropna()\nnonbookable = data.query(\"instant_bookable == 'f'\")[\"number_of_reviews\"].dropna()\n\naxes[7].boxplot([bookable, nonbookable])\naxes[7].set_title(\"Reviews by Instant Bookability\")\naxes[7].set_xticks([1, 2])\naxes[7].set_xticklabels([\"Instant Bookable\", \"Not Instant Bookable\"])\naxes[7].set_ylabel(\"Number of Reviews\")\n\n# number_of_reviews\naxes[8].boxplot(reviews)\naxes[8].set_title(\"Distribution of Number of Reviews\")\naxes[8].set_ylabel(\"Number of Reviews\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n1. Number of Days Listed - The distribution is highly right-skewed, with clear outliers (listings exceeding 40,000 days). - Most listings have been active for fewer than 2,000 days. Consider applying a transformation before modeling.\n2. Bathrooms and Bedrooms - Most listings have 1 bathroom and 1–2 bedrooms. - A small number of listings show 0 bedrooms, and a few show over 5 bedrooms.\n3. Price - Price is also extremely right-skewed, with some listings charging over $10,000 per night. - A log transformation of price is strongly recommended to reduce the influence of outliers in downstream models.\n4. Review Scores (Cleanliness, Location, Value) - All three review scores are tightly clustered between 8 and 10, indicating a ceiling effect in guest ratings. - Despite limited variation, these variables may still capture meaningful differences.\n5. Instant Bookability - Listings that are instantly bookable tend to receive more reviews on average. - The boxplot shows higher median and upper quartile values for instantly bookable listings, suggesting greater guest engagement or booking volume.\n6. Number of Reviews - Review counts are highly right-skewed. Most listings receive fewer than 50 reviews. - This justifies using a Poisson or negative binomial model for count data, and suggests that trimming or transforming extreme values may improve model stability.\nWhen checking missing values, there are several columns with missing values, including host_since, bathrooms, bedrooms, review_scores_cleanliness, review_scores_location, review_scores_value.\nExtract the columns needed and check missing values\n\n\nCode\ndata_v2 = data[\n    [\n        \"days\",\n        \"room_type\",\n        \"bathrooms\",\n        \"bedrooms\",\n        \"price\",\n        \"number_of_reviews\",\n        \"review_scores_cleanliness\",\n        \"review_scores_location\",\n        \"review_scores_value\",\n        \"instant_bookable\",\n    ]\n].copy()\n\nprint(data_v2.isna().sum())\n\n\ndays                             0\nroom_type                        0\nbathrooms                      160\nbedrooms                        76\nprice                            0\nnumber_of_reviews                0\nreview_scores_cleanliness    10195\nreview_scores_location       10254\nreview_scores_value          10256\ninstant_bookable                 0\ndtype: int64\n\n\nObserve the distribution of columns\n\n\nCode\ndata_v2.describe().round(1)\n\n\n\n\n\n\n\n\n\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n40628.0\n40468.0\n40552.0\n40628.0\n40628.0\n30433.0\n30374.0\n30372.0\n\n\nmean\n1102.4\n1.1\n1.1\n144.8\n15.9\n9.2\n9.4\n9.3\n\n\nstd\n1383.3\n0.4\n0.7\n210.7\n29.2\n1.1\n0.8\n0.9\n\n\nmin\n1.0\n0.0\n0.0\n10.0\n0.0\n2.0\n2.0\n2.0\n\n\n25%\n542.0\n1.0\n1.0\n70.0\n1.0\n9.0\n9.0\n9.0\n\n\n50%\n996.0\n1.0\n1.0\n100.0\n4.0\n10.0\n10.0\n10.0\n\n\n75%\n1535.0\n1.0\n1.0\n170.0\n17.0\n10.0\n10.0\n10.0\n\n\nmax\n42828.0\n8.0\n10.0\n10000.0\n421.0\n10.0\n10.0\n10.0\n\n\n\n\n\n\n\nAfter discovering the distribution of columns, I decided to fill in missing values with medians and means.\n\nMedians (bathrooms and bedrooms): These two columns only have a few rows without values and most rows have reords with 1.(25%, 50%, 75%)\nMeans (review_scores_cleanliness, review_scores_location, and review_scores_value): These columns have a quarter of rows without values. To avoid effecting the results, I decided to fill in the columns with mean, instead of dropping them directly.\n\nFill in missing values\n\n\nCode\nmean_cols = [\n    \"review_scores_cleanliness\",\n    \"review_scores_location\",\n    \"review_scores_value\",\n]\ndata_v2[mean_cols] = data_v2[mean_cols].fillna(data_v2[mean_cols].mean())\nmedian_cols = [\"bathrooms\", \"bedrooms\"]\ndata_v2[median_cols] = data_v2[median_cols].fillna(data_v2[median_cols].mean())\ndata_v2.describe().round(1)\n\n\n\n\n\n\n\n\n\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n40628.0\n40628.0\n40628.0\n40628.0\n40628.0\n40628.0\n40628.0\n40628.0\n\n\nmean\n1102.4\n1.1\n1.1\n144.8\n15.9\n9.2\n9.4\n9.3\n\n\nstd\n1383.3\n0.4\n0.7\n210.7\n29.2\n1.0\n0.7\n0.8\n\n\nmin\n1.0\n0.0\n0.0\n10.0\n0.0\n2.0\n2.0\n2.0\n\n\n25%\n542.0\n1.0\n1.0\n70.0\n1.0\n9.0\n9.0\n9.0\n\n\n50%\n996.0\n1.0\n1.0\n100.0\n4.0\n9.2\n9.4\n9.3\n\n\n75%\n1535.0\n1.0\n1.0\n170.0\n17.0\n10.0\n10.0\n10.0\n\n\nmax\n42828.0\n8.0\n10.0\n10000.0\n421.0\n10.0\n10.0\n10.0\n\n\n\n\n\n\n\nFeature cleaning and engineering\n\nMark instant_bookable as: “t” = 1, “f” = 0\nLog-transformation on price\n\n\n\nCode\ndf_model = data_v2.copy()\ndf_model[\"instant_bookable\"] = df_model[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\ndf_model[\"log_price\"] = np.log(df_model[\"price\"] + 1)\n\ndf_model = df_model[[\n        \"number_of_reviews\", \"log_price\", \"days\", \"bathrooms\", \"bedrooms\",\n        \"review_scores_cleanliness\", \"review_scores_location\",\n        \"review_scores_value\", \"instant_bookable\"]]\ndf_model.head(3)\n\n\n\n\n\n\n\n\n\nnumber_of_reviews\nlog_price\ndays\nbathrooms\nbedrooms\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n1\n150\n4.094345\n3130\n1.0\n1.0\n9.00000\n9.000000\n9.000000\n0\n\n\n2\n20\n5.442418\n3127\n1.0\n0.0\n9.00000\n10.000000\n9.000000\n0\n\n\n3\n0\n5.017280\n3050\n1.0\n1.0\n9.19837\n9.413544\n9.331522\n0\n\n\n\n\n\n\n\nBuild Poisson Regression Model\nPoisson regression result (using sm.GLM()):\n\n\nCode\nX = sm.add_constant(df_model.drop(columns=\"number_of_reviews\")).astype(float)\nY = df_model[\"number_of_reviews\"].astype(int)\n\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresult = model.fit()\n\nsummary_table = pd.DataFrame(\n    {\n        \"coef\": result.params,\n        \"std err\": result.bse,\n        \"z\": result.tvalues,\n        \"P&gt;|z|\": result.pvalues,\n        \"[0.025\": result.conf_int()[0],\n        \"0.975]\": result.conf_int()[1],\n    }\n)\n\nsummary_table.round(3)\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nconst\n3.161\n0.019\n170.603\n0.0\n3.124\n3.197\n\n\nlog_price\n0.067\n0.002\n31.961\n0.0\n0.063\n0.071\n\n\ndays\n0.000\n0.000\n142.552\n0.0\n0.000\n0.000\n\n\nbathrooms\n-0.160\n0.004\n-42.804\n0.0\n-0.167\n-0.153\n\n\nbedrooms\n0.067\n0.002\n33.320\n0.0\n0.063\n0.071\n\n\nreview_scores_cleanliness\n0.146\n0.002\n85.226\n0.0\n0.143\n0.149\n\n\nreview_scores_location\n-0.117\n0.002\n-64.318\n0.0\n-0.121\n-0.114\n\n\nreview_scores_value\n-0.107\n0.002\n-52.865\n0.0\n-0.111\n-0.103\n\n\ninstant_bookable\n0.367\n0.003\n127.747\n0.0\n0.362\n0.373\n\n\n\n\n\n\n\n1. Price (log-transformed) - Higher prices are associated with more reviews. - A 1% increase in price leads to an estimated 6.9% increase in expected reviews.\n2. Days listed on Airbnb - Statistically significant, but effect per day is minimal as the coefficient of the variable is extremely close to 0. - However, still relevant over long durations.\n3. Bathrooms - More bathrooms are associated with fewer reviews. - May reflect luxury/niche listings that are less frequently booked.\n4. Bedrooms - Each additional bedroom increases expected reviews by about 6.9%. - Suggests that larger listings accommodate more guests, driving more bookings and reviews.\n5. Review Score: Cleanliness - Very strong positive effect: a one-point increase predicts a 15.7% rise in reviews. - Cleanliness clearly matters to guest engagement.\n6. Review Score: Location - Surprisingly negative: higher location scores are associated with fewer reviews. - May reflect higher-end listings in quieter markets or less turnover.\n7. Review Score: Value - Also negatively associated with reviews. - Possibly indicates that guests feeling they got “great value” may not feel compelled to leave a review.\n8. Instant Bookable - Listings with instant booking enabled receive 44.3% more reviews than those without. - Suggests ease of booking directly drives guest conversion and engagement.\nConclusion\nIn conclusion, all features included in the Poisson regression model exhibit statistically significant effects on the number of reviews, which we use as a proxy for customer engagement. While most variables, such as price, number of bedrooms, cleanliness score, and instant bookability, show positive associations with review counts, a few results stand out as counterintuitive:\n\nNumber of bathrooms,\nReview score for location, and\nReview score for value\n\nThese variables are all negatively associated with the number of reviews.\nOne possible explanation is that higher-end listings, which tend to have more bathrooms and receive higher quality scores, may also experience lower guest turnover, therefore, generate fewer reviews. This assumption requires further investigation into whether these listings differ systematically in actual booking frequency or in the sentiment of their reviews."
  }
]